{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a384c3-9d5a-4c50-b993-35fd89f8a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a77e6ff-20f1-421b-a63f-44842257de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd156c1-b5db-4c96-b56d-ae535449810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476bbd24-0bd9-4646-9784-01ee912ca752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91cfbacb-63fc-495a-82e7-0fc4c6c663c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-----+\n",
      "|transaction_id|user_id|amount| location|transaction_time|tr_us|\n",
      "+--------------+-------+------+---------+----------------+-----+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00| 2011|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04| 2021|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00| 2032|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01| 2042|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 2053|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34| 2063|\n",
      "+--------------+-------+------+---------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('tr_us', functions.concat(col('transaction_id'), col('user_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c84794-62c1-4c4e-bb5c-4e8c7ba0d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-----+\n",
      "|transaction_id|user_id|amount| location|transaction_time|tr_us|\n",
      "+--------------+-------+------+---------+----------------+-----+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|  201|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|  202|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|  203|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|  204|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|  205|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|  206|\n",
      "+--------------+-------+------+---------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('tr_us', functions.concat(col('transaction_id'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27abb665-5db9-4ccd-ba5b-c43dbeb9d0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+------+\n",
      "|transaction_id|user_id|amount| location|transaction_time| tr_us|\n",
      "+--------------+-------+------+---------+----------------+------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|201123|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|202123|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|203123|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|204123|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|205123|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|206123|\n",
      "+--------------+-------+------+---------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('tr_us', functions.concat(col('transaction_id'),functions.lit('123'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187f2662-35c7-4103-9967-1411025aafcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|  tr_us|\n",
      "+--------------+-------+------+---------+----------------+-------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2011123|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2021123|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2032123|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2042123|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2053123|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2063123|\n",
      "+--------------+-------+------+---------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('tr_us', functions.concat('transaction_id','user_id',functions.lit('123'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf0637fd-c9e3-47f8-bcf5-10e61d9c46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      tr_us|\n",
      "+--------------+-------+------+---------+----------------+-----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|201==1==123|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|202==1==123|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|203==2==123|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|204==2==123|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|205==3==123|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|206==3==123|\n",
      "+--------------+-------+------+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('tr_us', functions.concat_ws('==','transaction_id','user_id',functions.lit('123'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b46b178-dd54-4867-8277-25303f2845c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|upper_conv|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00| HYDERABAD|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04| HYDERABAD|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00| HYDERABAD|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|   CHENNAI|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|  BANGLORE|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34| HYDERABAD|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('upper_conv', functions.upper('location')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6dcff48-53ad-4c53-a63b-3d691b315d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|upper_conv|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|       201|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|       202|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|       203|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|       204|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|       205|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|       206|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('upper_conv', functions.upper('transaction_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fa3142-6ca5-4e6f-8a01-6f18221de0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|upper_conv|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|         3|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|         3|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|         3|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|         3|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|         3|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|         3|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df.withColumn('upper_conv', functions.length('transaction_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d705e8-5f26-4954-9ab4-c4b6650c68bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+---------------+---------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       trim_ext|     after_trim|\n",
      "+--------------+-------+------+---------+----------------+---------------+---------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|123Hyderabad213|123Hyderabad213|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|123Hyderabad213|123Hyderabad213|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|123Hyderabad213|123Hyderabad213|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|  123Chennai213|  123Chennai213|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 123Banglore213| 123Banglore213|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|123Hyderabad213|123Hyderabad213|\n",
      "+--------------+-------+------+---------+----------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql import functions\n",
    "df = df.withColumn('trim_ext', functions.concat(functions.lit('123'),\"location\", functions.lit('213')))\n",
    "df.withColumn('after_trim', functions.trim('trim_ext')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7606a71f-efa8-44e9-b5e4-3d3db4827e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+--------------------+--------------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|            trim_ext|          after_trim|\n",
      "+--------------+-------+------+---------+----------------+--------------------+--------------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00| 123   Hyderabad ...|123   Hyderabad  ...|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04| 123   Hyderabad ...|123   Hyderabad  ...|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00| 123   Hyderabad ...|123   Hyderabad  ...|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01| 123   Chennai   ...| 123   Chennai   213|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 123   Banglore  ...|123   Banglore   213|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34| 123   Hyderabad ...|123   Hyderabad  ...|\n",
      "+--------------+-------+------+---------+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql import functions\n",
    "df = df.withColumn('trim_ext', functions.concat(functions.lit(' 123   '),\"location\", functions.lit('   213 ')))\n",
    "df.withColumn('after_trim', functions.trim('trim_ext')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ea85b58-b5b1-423b-bb46-c9eee4e40172",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trim() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrim_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, functions\u001b[38;5;241m.\u001b[39mconcat(functions\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m123\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, functions\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m213\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m----> 4\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_trim\u001b[39m\u001b[38;5;124m'\u001b[39m, functions\u001b[38;5;241m.\u001b[39mtrim(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrim_ext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: trim() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql import functions\n",
    "df = df.withColumn('trim_ext', functions.concat(functions.lit('123'),\"location\", functions.lit('213')))\n",
    "df.withColumn('after_trim', functions.trim('trim_ext','3')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08014cb6-f37d-447a-b918-5694db27e45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ColumnOrName'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Trim the spaces from both ends for the specified string column.\n",
       "\n",
       ".. versionadded:: 1.5.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "col : :class:`~pyspark.sql.Column` or str\n",
       "    target column to work on.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`~pyspark.sql.Column`\n",
       "    trimmed values from both sides.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
       ">>> df.select(trim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
       "+-----+------+\n",
       "|    r|length|\n",
       "+-----+------+\n",
       "|Spark|     5|\n",
       "|Spark|     5|\n",
       "|Spark|     5|\n",
       "+-----+------+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "functions.trim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a60cc5-febd-4ae5-aa6c-6284d3549e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+---------------+---------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       trim_ext|     after_trim|\n",
      "+--------------+-------+------+---------+----------------+---------------+---------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|123Hyderabad213|123Hyderabad213|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|123Hyderabad213|123Hyderabad213|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|123Hyderabad213|123Hyderabad213|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|  123Chennai213|  123Chennai213|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 123Banglore213| 123Banglore213|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|123Hyderabad213|123Hyderabad213|\n",
      "+--------------+-------+------+---------+----------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql import functions\n",
    "df = df.withColumn('trim_ext', functions.concat(functions.lit('123'),\"location\", functions.lit('213')))\n",
    "df.withColumn('after_trim', functions.ltrim('trim_ext')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af68144-51e3-4601-8975-b49df24f10d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mltrim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ColumnOrName'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Trim the spaces from left end for the specified string value.\n",
       "\n",
       ".. versionadded:: 1.5.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "col : :class:`~pyspark.sql.Column` or str\n",
       "    target column to work on.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`~pyspark.sql.Column`\n",
       "    left trimmed values.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
       ">>> df.select(ltrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
       "+-------+------+\n",
       "|      r|length|\n",
       "+-------+------+\n",
       "|  Spark|     5|\n",
       "|Spark  |     7|\n",
       "|  Spark|     5|\n",
       "+-------+------+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "functions.ltrim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac294aab-b42b-4d76-a28d-55ddcf05ffbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrtrim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ColumnOrName'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Trim the spaces from right end for the specified string value.\n",
       "\n",
       ".. versionadded:: 1.5.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "col : :class:`~pyspark.sql.Column` or str\n",
       "    target column to work on.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`~pyspark.sql.Column`\n",
       "    right trimmed values.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
       ">>> df.select(rtrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
       "+--------+------+\n",
       "|       r|length|\n",
       "+--------+------+\n",
       "|   Spark|     8|\n",
       "|   Spark|     5|\n",
       "|   Spark|     6|\n",
       "+--------+------+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "functions.rtrim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9c8b765-547c-4df4-b61f-1459a7200e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lpad', 'rpad']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in dir(functions) if 'pad' in i.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85bcdf3e-fd85-46e0-b42d-9df8cda9389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+---------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       trim_ext|\n",
      "+--------------+-------+------+---------+----------------+---------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|123Hyderabad213|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|123Hyderabad213|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|123Hyderabad213|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|  123Chennai213|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 123Banglore213|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|123Hyderabad213|\n",
      "+--------------+-------+------+---------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "from pyspark.sql import functions\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39b2720c-266e-416f-8d77-4f9286164dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+---------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       trim_ext|after_lpad|\n",
      "+--------------+-------+------+---------+----------------+---------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|123Hyderabad213|#Hyderabad|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|123Hyderabad213|#Hyderabad|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|123Hyderabad213|#Hyderabad|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|  123Chennai213|###Chennai|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 123Banglore213|##Banglore|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|123Hyderabad213|#Hyderabad|\n",
      "+--------------+-------+------+---------+----------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('after_lpad', functions.lpad('location',10,\"#\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2de95c46-261a-4b14-815a-e731032fdbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+---------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       trim_ext|after_lpad|\n",
      "+--------------+-------+------+---------+----------------+---------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|123Hyderabad213|    Hydera|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|123Hyderabad213|    Hydera|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|123Hyderabad213|    Hydera|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|  123Chennai213|    Chenna|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30| 123Banglore213|    Banglo|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|123Hyderabad213|    Hydera|\n",
      "+--------------+-------+------+---------+----------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('after_lpad', functions.lpad('location',6,\"#\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45f907fa-f417-4bb8-8539-711b6ec72e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e113a0b-df55-45ea-9ca3-1ceb4570255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+--------------------+\n",
      "|customer_id|               items|      date|         items_split|\n",
      "+-----------+--------------------+----------+--------------------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop, Mouse, K...|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|[Smartphone, Earb...|\n",
      "|        103|              Tablet|2025-03-07|            [Tablet]|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor, HDMICab...|\n",
      "+-----------+--------------------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.withColumn('items_split',functions.split('items',';')).show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34d617c6-fbed-4c25-8678-6410bc5688a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ColumnOrName'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Splits str around matches of the given pattern.\n",
       "\n",
       ".. versionadded:: 1.5.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "str : :class:`~pyspark.sql.Column` or str\n",
       "    a string expression to split\n",
       "pattern : str\n",
       "    a string representing a regular expression. The regex string should be\n",
       "    a Java regular expression.\n",
       "limit : int, optional\n",
       "    an integer which controls the number of times `pattern` is applied.\n",
       "\n",
       "    * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
       "                     resulting array's last entry will contain all input beyond the last\n",
       "                     matched pattern.\n",
       "    * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
       "                      array can be of any size.\n",
       "\n",
       "    .. versionchanged:: 3.0\n",
       "       `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`~pyspark.sql.Column`\n",
       "    array of separated strings.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
       ">>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
       "[Row(s=['one', 'twoBthreeC'])]\n",
       ">>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
       "[Row(s=['one', 'two', 'three', ''])]\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\functions.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "functions.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33aefa57-38b2-4591-aba2-d20dd242245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+--------------------+\n",
      "|customer_id|               items|      date|         items_split|\n",
      "+-----------+--------------------+----------+--------------------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop;Mouse;Key...|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|[Smartphone;Earbuds]|\n",
      "|        103|              Tablet|2025-03-07|            [Tablet]|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor;HDMICabl...|\n",
      "+-----------+--------------------+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.withColumn('items_split',functions.split('items',';',1)).show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35c0cb69-b68a-4f78-8cd2-c0f131894620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+----------+---------------------------+\n",
      "|customer_id|items                    |date      |items_split                |\n",
      "+-----------+-------------------------+----------+---------------------------+\n",
      "|101        |Laptop;Mouse;Keyboard    |2025-03-09|[Laptop;Mouse;Keyboard]    |\n",
      "|102        |Smartphone;Earbuds       |2025-03-08|[Smartphone;Earbuds]       |\n",
      "|103        |Tablet                   |2025-03-07|[Tablet]                   |\n",
      "|104        |Monitor;HDMICable;Speaker|2025-03-06|[Monitor;HDMICable;Speaker]|\n",
      "+-----------+-------------------------+----------+---------------------------+\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.withColumn('items_split',functions.split('items',';',1)).show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "679e0cd2-c250-4313-b187-5f44056ceb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+----------+----------------------------+\n",
      "|customer_id|items                    |date      |items_split                 |\n",
      "+-----------+-------------------------+----------+----------------------------+\n",
      "|101        |Laptop;Mouse;Keyboard    |2025-03-09|[Laptop, Mouse;Keyboard]    |\n",
      "|102        |Smartphone;Earbuds       |2025-03-08|[Smartphone, Earbuds]       |\n",
      "|103        |Tablet                   |2025-03-07|[Tablet]                    |\n",
      "|104        |Monitor;HDMICable;Speaker|2025-03-06|[Monitor, HDMICable;Speaker]|\n",
      "+-----------+-------------------------+----------+----------------------------+\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.withColumn('items_split',functions.split('items',';',2)).show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c560e0-fbfe-4805-8c7f-35343fec288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0afe9d7-49ce-45c0-bc88-fb88e12f071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e41ebe8-1d9e-418a-84cd-dfa53fc6cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+--------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|        concat|\n",
      "+--------------+-------+------+---------+----------------+--------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|          NULL|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|          NULL|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00| 2200Hyderabad|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|          NULL|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|          NULL|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|31500Hyderabad|\n",
      "+--------------+-------+------+---------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.withColumn('concat',functions.concat('user_id','amount','location')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895a3f77-9699-4519-a0eb-1142cb7a8dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|         |10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|         |10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f194ed2-9dbc-4df7-b5df-38d71edb2eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|         |10-03-2025 12:00|\n",
      "|           202|      0|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|         |10-03-2025 14:01|\n",
      "|           205|      0|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"\").fillna(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7ad131-b2b7-40b2-807f-00a0a8eb3751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+--------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|        concat|\n",
      "+--------------+-------+------+---------+----------------+--------------+\n",
      "|           201|      1|   500|         |10-03-2025 12:00|          1500|\n",
      "|           202|      0|   700|Hyderabad|10-03-2025 12:04| 0700Hyderabad|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00| 2200Hyderabad|\n",
      "|           204|      2|   250|         |10-03-2025 14:01|          2250|\n",
      "|           205|      0|  1000| Banglore|10-03-2025 15:30| 01000Banglore|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|31500Hyderabad|\n",
      "+--------------+-------+------+---------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df = df.fillna(\"\").fillna(0)\n",
    "df.withColumn('concat',functions.concat('user_id','amount','location')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c3a785-f895-43ff-a4c6-797a7e92247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1930ecbd-d9a8-4f83-9a3b-81eedf436504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d4fe2f-f683-4b0c-a253-8979624a26a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MAXYEAR', 'MINYEAR', 'UTC', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'date', 'datetime', 'datetime_CAPI', 'time', 'timedelta', 'timezone', 'tzinfo']\n"
     ]
    }
   ],
   "source": [
    "print(dir(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0651d9-7434-45dc-97c3-0c61fe5bc96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__rsub__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', 'astimezone', 'combine', 'ctime', 'date', 'day', 'dst', 'fold', 'fromisocalendar', 'fromisoformat', 'fromordinal', 'fromtimestamp', 'hour', 'isocalendar', 'isoformat', 'isoweekday', 'max', 'microsecond', 'min', 'minute', 'month', 'now', 'replace', 'resolution', 'second', 'strftime', 'strptime', 'time', 'timestamp', 'timetuple', 'timetz', 'today', 'toordinal', 'tzinfo', 'tzname', 'utcfromtimestamp', 'utcnow', 'utcoffset', 'utctimetuple', 'weekday', 'year']\n"
     ]
    }
   ],
   "source": [
    "print(dir(datetime.datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a739a163-24c1-4d75-b510-5b0d31de8e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-14 00:00:00 <class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#\"2025-03-14\"\n",
    "da = datetime.strptime(\"2025-03-14\",\"%Y-%m-%d\")\n",
    "print(da, type(da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bbf711e-1b08-46cb-ace4-fa2874ee1e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '2025-03-14' does not match format '%m-%Y-%d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\"2025-03-14\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m da \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrptime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-03-14\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(da, \u001b[38;5;28mtype\u001b[39m(da))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\_strptime.py:554\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    552\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m _strptime(data_string, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    555\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    556\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\_strptime.py:333\u001b[0m, in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    331\u001b[0m found \u001b[38;5;241m=\u001b[39m format_regex\u001b[38;5;241m.\u001b[39mmatch(data_string)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found:\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime data \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match format \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    334\u001b[0m                      (data_string, \u001b[38;5;28mformat\u001b[39m))\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconverted data remains: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    337\u001b[0m                       data_string[found\u001b[38;5;241m.\u001b[39mend():])\n",
      "\u001b[1;31mValueError\u001b[0m: time data '2025-03-14' does not match format '%m-%Y-%d'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#\"2025-03-14\"\n",
    "da = datetime.strptime(\"2025-03-14\",\"%m-%Y-%d\")\n",
    "print(da, type(da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99875bb4-2597-41ea-9725-0f920a8201da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-14 00:00:00 <class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#\"2025-03-14\"\n",
    "da = datetime.strptime(\"2025-03-14\",\"%Y-%m-%d\")\n",
    "print(da, type(da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90c0aa07-2a3b-489d-a159-d396960c73cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-14 00:00:00 <class 'datetime.datetime'>\n",
      "03-14-2025 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#\"2025-03-14\"\n",
    "da = datetime.strptime(\"2025-03-14\",\"%Y-%m-%d\")\n",
    "print(da, type(da))\n",
    "da_str = datetime.strftime(da, '%m-%d-%Y')\n",
    "print(da_str, type(da_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cc053b9-0da6-4d74-8000-621d231819ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+----------+\n",
      "|customer_id|items                    |date      |\n",
      "+-----------+-------------------------+----------+\n",
      "|101        |Laptop;Mouse;Keyboard    |2025-03-09|\n",
      "|102        |Smartphone;Earbuds       |2025-03-08|\n",
      "|103        |Tablet                   |2025-03-07|\n",
      "|104        |Monitor;HDMICable;Speaker|2025-03-06|\n",
      "+-----------+-------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89c9d91c-df3c-46a5-af62-d82724c93c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+----------+-----------------------------+\n",
      "|customer_id|items                    |date      |items_list                   |\n",
      "+-----------+-------------------------+----------+-----------------------------+\n",
      "|101        |Laptop;Mouse;Keyboard    |2025-03-09|[Laptop, Mouse, Keyboard]    |\n",
      "|102        |Smartphone;Earbuds       |2025-03-08|[Smartphone, Earbuds]        |\n",
      "|103        |Tablet                   |2025-03-07|[Tablet]                     |\n",
      "|104        |Monitor;HDMICable;Speaker|2025-03-06|[Monitor, HDMICable, Speaker]|\n",
      "+-----------+-------------------------+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df.withColumn('items_list',functions.split(\"items\",\";\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92e0a83c-dc64-4513-8f77-9e1474c6f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------------------------+----------+\n",
      "|customer_id|date      |items_list                   |itm       |\n",
      "+-----------+----------+-----------------------------+----------+\n",
      "|101        |2025-03-09|[Laptop, Mouse, Keyboard]    |Laptop    |\n",
      "|101        |2025-03-09|[Laptop, Mouse, Keyboard]    |Mouse     |\n",
      "|101        |2025-03-09|[Laptop, Mouse, Keyboard]    |Keyboard  |\n",
      "|102        |2025-03-08|[Smartphone, Earbuds]        |Smartphone|\n",
      "|102        |2025-03-08|[Smartphone, Earbuds]        |Earbuds   |\n",
      "|103        |2025-03-07|[Tablet]                     |Tablet    |\n",
      "|104        |2025-03-06|[Monitor, HDMICable, Speaker]|Monitor   |\n",
      "|104        |2025-03-06|[Monitor, HDMICable, Speaker]|HDMICable |\n",
      "|104        |2025-03-06|[Monitor, HDMICable, Speaker]|Speaker   |\n",
      "+-----------+----------+-----------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\explode_data.txt\", header=True, inferSchema=True)\n",
    "df = df.withColumn('items_list',functions.split(\"items\",\";\"))\n",
    "df = df.drop('items')\n",
    "\n",
    "df.withColumn('itm', functions.explode(\"items_list\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f6757d0-368c-47a4-bad7-fafdba5d4fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdbf26af-80fb-47cf-8df1-7888cc3f9fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna(\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e149028-4b16-4aa0-9bda-8139fde33090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna(subset=['user_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20425bbd-872a-4ae5-837b-205f0458e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna(subset=['user_id','location']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5462dc2-0e87-4d82-83d2-12718c78d055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mhow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'any'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mthresh\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msubset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` omitting rows with null values.\n",
       ":func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
       "\n",
       ".. versionadded:: 1.3.1\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "how : str, optional\n",
       "    'any' or 'all'.\n",
       "    If 'any', drop a row if it contains any nulls.\n",
       "    If 'all', drop a row only if all its values are null.\n",
       "thresh: int, optional\n",
       "    default None\n",
       "    If specified, drop rows that have less than `thresh` non-null values.\n",
       "    This overwrites the `how` parameter.\n",
       "subset : str, tuple or list, optional\n",
       "    optional list of column names to consider.\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    DataFrame with null only rows excluded.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql import Row\n",
       ">>> df = spark.createDataFrame([\n",
       "...     Row(age=10, height=80, name=\"Alice\"),\n",
       "...     Row(age=5, height=None, name=\"Bob\"),\n",
       "...     Row(age=None, height=None, name=\"Tom\"),\n",
       "...     Row(age=None, height=None, name=None),\n",
       "... ])\n",
       ">>> df.na.drop().show()\n",
       "+---+------+-----+\n",
       "|age|height| name|\n",
       "+---+------+-----+\n",
       "| 10|    80|Alice|\n",
       "+---+------+-----+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc759fcb-5469-4f4a-9078-aaa2a7f7dca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna(how=\"any\", thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60054bdd-efbc-41ee-99c1-67bfa0ceedb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna(how=\"any\", thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c8e5011-6715-4ed6-983c-291fdc92efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           202|   NULL|   700|     NULL|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "|          NULL|   NULL|  NULL|     NULL|            NULL|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|     NULL|10-03-2025 12:00|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|     NULL|10-03-2025 14:01|\n",
      "|           205|   NULL|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions1.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.dropna(thresh=4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042873a3-4ca0-4739-ab24-2aa559535514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
