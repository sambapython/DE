{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb599b5-0138-434f-9770-6452b2585cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d416b0c-bcbe-41eb-8d47-dd79cf129f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['curdate', 'current_date', 'date_add', 'date_diff', 'date_format', 'date_from_unix_date', 'date_part', 'date_sub', 'date_trunc', 'dateadd', 'datediff', 'datepart', 'make_date', 'to_date', 'unix_date']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in dir(functions) if 'date' in i.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba11063c-1af2-42de-861c-4a673fb6a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convert_timezone', 'current_timestamp', 'current_timezone', 'from_unixtime', 'from_utc_timestamp', 'localtimestamp', 'make_timestamp', 'make_timestamp_ltz', 'make_timestamp_ntz', 'timestamp_micros', 'timestamp_millis', 'timestamp_seconds', 'to_timestamp', 'to_timestamp_ltz', 'to_timestamp_ntz', 'to_unix_timestamp', 'to_utc_timestamp', 'try_to_timestamp', 'unix_timestamp', 'window_time']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in dir(functions) if 'time' in i.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdf8f43-ef1a-4a7e-a361-ab58e19bd0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554d51bd-0ff8-4b9d-853c-757d48e953ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----+\n",
      "|transaction_id|user_id|amount| location|transaction_time|date|\n",
      "+--------------+-------+------+---------+----------------+----+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|NULL|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|NULL|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|NULL|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|NULL|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|NULL|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|NULL|\n",
      "+--------------+-------+------+---------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.withColumn(\"date\", functions.date_format('transaction_time', 'yyyy-MM-dd HH:mm')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3db5642-cc81-4768-8e72-d44bbf04dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|\n",
      "+--------------+-------+------+---------+----------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"date\", functions.to_date('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224fb4b9-e920-4381-8ec6-912bce5b8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|add_2_days|\n",
      "+--------------+-------+------+---------+----------------+----------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-05|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-05|\n",
      "+--------------+-------+------+---------+----------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- add_2_days: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"date\", functions.to_date('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_days\", functions.date_add('date',2))\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25770aca-c111-412b-a05b-4535d9a6c576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|     date_timestamp|add_2_days|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-03 12:00:00|2025-10-05|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-03 12:04:00|2025-10-05|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- add_2_days: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"date\", functions.to_date('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"date_timestamp\", functions.to_timestamp('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_days\", functions.date_add('date',2))\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9360c092-459d-45a8-92dd-8e58b9b5d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|     date_timestamp|add_2_days|           timestamp|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-03 12:00:00|2025-10-05|2025-03-11 10:25:...|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-03 12:04:00|2025-10-05|2025-03-11 10:25:...|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- add_2_days: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"date\", functions.to_date('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"date_timestamp\", functions.to_timestamp('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_days\", functions.date_add('date',2))\n",
    "df = df.withColumn('timestamp',functions.current_timestamp())\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb16b0b-b174-43eb-9216-66a6b4155f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|     date_timestamp|add_2_days|           timestamp|date_diff_col|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-03 12:00:00|2025-10-05|2025-03-11 10:32:...|           -2|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-03 12:04:00|2025-10-05|2025-03-11 10:32:...|           -2|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- add_2_days: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      " |-- date_diff_col: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show(2)\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"date\", functions.to_date('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"date_timestamp\", functions.to_timestamp('transaction_time', 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_days\", functions.date_add('date',2))\n",
    "df = df.withColumn('timestamp',functions.current_timestamp())\n",
    "df = df.withColumn('date_diff_col', functions.date_diff(\"date\",\"add_2_days\") )\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c99f99-00a7-4336-8907-7e8a0b51eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|     date_timestamp|add_2_days|           timestamp|date_diff_col|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-03 12:00:00|2025-10-05|2025-03-11 10:53:...|           -2|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-03 12:04:00|2025-10-05|2025-03-11 10:53:...|           -2|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03|2025-10-03 14:00:00|2025-10-05|2025-03-11 10:53:...|           -2|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03|2025-10-03 14:01:00|2025-10-05|2025-03-11 10:53:...|           -2|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03|2025-10-03 15:30:00|2025-10-05|2025-03-11 10:53:...|           -2|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03|2025-10-03 15:34:00|2025-10-05|2025-03-11 10:53:...|           -2|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- add_2_days: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      " |-- date_diff_col: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b8b8a6e-f41a-4795-baf3-7eaf1cc3e93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+--------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|     date_timestamp|add_2_days|           timestamp|date_diff_col|location_upper|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+--------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-03 12:00:00|2025-10-05|2025-03-11 10:54:...|           -2|     HYDERABAD|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-03 12:04:00|2025-10-05|2025-03-11 10:54:...|           -2|     HYDERABAD|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03|2025-10-03 14:00:00|2025-10-05|2025-03-11 10:54:...|           -2|     HYDERABAD|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03|2025-10-03 14:01:00|2025-10-05|2025-03-11 10:54:...|           -2|       CHENNAI|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03|2025-10-03 15:30:00|2025-10-05|2025-03-11 10:54:...|           -2|      BANGLORE|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03|2025-10-03 15:34:00|2025-10-05|2025-03-11 10:54:...|           -2|     HYDERABAD|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+--------------+\n",
      "\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|      date|     date_timestamp|add_2_days|           timestamp|date_diff_col|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03|2025-10-03 12:00:00|2025-10-05|2025-03-11 10:54:...|           -2|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03|2025-10-03 12:04:00|2025-10-05|2025-03-11 10:54:...|           -2|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03|2025-10-03 14:00:00|2025-10-05|2025-03-11 10:54:...|           -2|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03|2025-10-03 14:01:00|2025-10-05|2025-03-11 10:54:...|           -2|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03|2025-10-03 15:30:00|2025-10-05|2025-03-11 10:54:...|           -2|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03|2025-10-03 15:34:00|2025-10-05|2025-03-11 10:54:...|           -2|\n",
      "+--------------+-------+------+---------+----------------+----------+-------------------+----------+--------------------+-------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- add_2_days: date (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      " |-- date_diff_col: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('location_upper', functions.upper('location')).show()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b8732-b7c9-4a61-815f-64d3d4052ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('location_upper', functions.upper('transaction_id')).show()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
