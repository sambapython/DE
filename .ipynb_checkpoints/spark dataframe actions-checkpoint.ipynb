{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42bd627b-7099-4ea0-8281-be848686de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8901502d-137d-48d3-9699-65f2765d2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b10bf8-9976-47f8-bd21-48c8a52d7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+\n",
      "|salary_id| emp_id|salary_month|  amount|\n",
      "+---------+-------+------------+--------+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|\n",
      "|  4504018|7087500|        NULL| 9370.35|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|\n",
      "|  4504022|7087520|  2020-03-20| 6668.04|\n",
      "|  4504023|7087520|  2020-04-19|  5948.5|\n",
      "|  4504024|7087520|  2020-05-19| 6760.61|\n",
      "|  4504025|7087520|  2020-06-18|12243.51|\n",
      "|  4504026|7087520|  2020-07-18| 3069.33|\n",
      "+---------+-------+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\salary50.txt\", header=True, inferSchema=True)\n",
    "print(df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05079c6f-d54b-4c04-bd75-bb3d5e3ad76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[emp_id: int, salary_month: date, amount: double]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\salary50.txt\", header=True, inferSchema=True)\n",
    "req_df = df.select(\"emp_id\",\"salary_month\",\"amount\")\n",
    "print(req_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf66663-a0d3-4eec-9b17-20705b47744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n",
      "| emp_id|salary_month|  amount|\n",
      "+-------+------------+--------+\n",
      "|7087500|  2024-09-25|    NULL|\n",
      "|7087500|        NULL| 9370.35|\n",
      "|   NULL|  2024-11-24| 3679.19|\n",
      "|7087500|  2024-12-24|  7730.6|\n",
      "|7087520|  2020-02-19|14822.82|\n",
      "|7087520|  2020-03-20| 6668.04|\n",
      "|7087520|  2020-04-19|  5948.5|\n",
      "|7087520|  2020-05-19| 6760.61|\n",
      "|7087520|  2020-06-18|12243.51|\n",
      "|7087520|  2020-07-18| 3069.33|\n",
      "+-------+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\salary50.txt\", header=True, inferSchema=True)\n",
    "req_df = df.select(\"emp_id\",\"salary_month\",\"amount\")\n",
    "req_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2228cb2-d20d-404b-b60b-5dc68b788ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'agg', 'alias', 'amount', 'approxQuantile', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'emp_id', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'salary_month', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
     ]
    }
   ],
   "source": [
    "print(dir(req_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f44dcab-23af-4aae-bfc8-a0b3037b5b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emp_id', 'salary_month', 'amount']\n"
     ]
    }
   ],
   "source": [
    "#emp_id|salary_month|amount\n",
    "# spark dataframes are IMMUTABLE\n",
    "req_df.withColumnRenamed(\"salary_month\",\"date\")\n",
    "print(req_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c32cdad-fc91-49be-bd48-699e8ad20a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emp_id', 'date', 'amount']\n"
     ]
    }
   ],
   "source": [
    "#emp_id|salary_month|amount\n",
    "# spark dataframes are IMMUTABLE\n",
    "req_df=req_df.withColumnRenamed(\"salary_month\",\"date\")\n",
    "print(req_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c81f5f-a1fd-484f-b8f1-94dbf9c28559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "| emp_id|      date|  amount|\n",
      "+-------+----------+--------+\n",
      "|7087500|2024-09-25|    NULL|\n",
      "|7087500|      NULL| 9370.35|\n",
      "|   NULL|2024-11-24| 3679.19|\n",
      "|7087500|2024-12-24|  7730.6|\n",
      "|7087520|2020-02-19|14822.82|\n",
      "|7087520|2020-03-20| 6668.04|\n",
      "|7087520|2020-04-19|  5948.5|\n",
      "|7087520|2020-05-19| 6760.61|\n",
      "|7087520|2020-06-18|12243.51|\n",
      "|7087520|2020-07-18| 3069.33|\n",
      "+-------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6cfb53c-4f80-4052-ba87-2a209429b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "| emp_id|      Date|  amount|\n",
      "+-------+----------+--------+\n",
      "|7087500|2024-09-25|    NULL|\n",
      "|7087500|      NULL| 9370.35|\n",
      "|   NULL|2024-11-24| 3679.19|\n",
      "|7087500|2024-12-24|  7730.6|\n",
      "|7087520|2020-02-19|14822.82|\n",
      "|7087520|2020-03-20| 6668.04|\n",
      "|7087520|2020-04-19|  5948.5|\n",
      "|7087520|2020-05-19| 6760.61|\n",
      "|7087520|2020-06-18|12243.51|\n",
      "|7087520|2020-07-18| 3069.33|\n",
      "+-------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.withColumnRenamed(\"date\",\"Date\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8dd9b35-3264-4a1e-9423-aa383e73ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "| emp_id|      date|  amount|\n",
      "+-------+----------+--------+\n",
      "|7087500|2024-09-25|    NULL|\n",
      "|7087500|      NULL| 9370.35|\n",
      "|   NULL|2024-11-24| 3679.19|\n",
      "|7087500|2024-12-24|  7730.6|\n",
      "|7087520|2020-02-19|14822.82|\n",
      "|7087520|2020-03-20| 6668.04|\n",
      "|7087520|2020-04-19|  5948.5|\n",
      "|7087520|2020-05-19| 6760.61|\n",
      "|7087520|2020-06-18|12243.51|\n",
      "|7087520|2020-07-18| 3069.33|\n",
      "+-------+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e09cb1-ec59-4393-900f-308382042a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "s=\"abcd\"\n",
    "s.upper()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091a5edf-ee97-4c5f-9b98-12e1e2d11a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCD\n"
     ]
    }
   ],
   "source": [
    "s=\"abcd\"\n",
    "s=s.upper()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b08118-23e0-4d51-8ee7-fa6a2042bbe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got str.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m req_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNULL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5172\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5130\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[0;32m   5131\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5169\u001b[0m \u001b[38;5;124;03m+---+-----+----+\u001b[39;00m\n\u001b[0;32m   5170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m-> 5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[0;32m   5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_COLUMN] Argument `col` should be a Column, got str."
     ]
    }
   ],
   "source": [
    "req_df.withColumn(\"year\", \"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6cbfb6-44c0-4420-9aad-f99863c44b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[emp_id: int, date: date, amount: double, year: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "req_df.withColumn(\"year\", lit(\"NULL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a7e8ed6-49d3-4f24-932a-4fd8e839a46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emp_id', 'date', 'amount']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc53bb82-b1c3-41ba-8cb1-19f4d8bc193d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----+\n",
      "| emp_id|      date|  amount|year|\n",
      "+-------+----------+--------+----+\n",
      "|7087500|2024-09-25|    NULL|NULL|\n",
      "|7087500|      NULL| 9370.35|NULL|\n",
      "|   NULL|2024-11-24| 3679.19|NULL|\n",
      "|7087500|2024-12-24|  7730.6|NULL|\n",
      "|7087520|2020-02-19|14822.82|NULL|\n",
      "|7087520|2020-03-20| 6668.04|NULL|\n",
      "|7087520|2020-04-19|  5948.5|NULL|\n",
      "|7087520|2020-05-19| 6760.61|NULL|\n",
      "|7087520|2020-06-18|12243.51|NULL|\n",
      "|7087520|2020-07-18| 3069.33|NULL|\n",
      "+-------+----------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "req_df = req_df.withColumn(\"year\", lit(\"NULL\"))\n",
    "print(req_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a1749d-7121-4664-9159-3d6077094458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----+\n",
      "| emp_id|      date|  amount|year|\n",
      "+-------+----------+--------+----+\n",
      "|7087500|2024-09-25|    NULL|2024|\n",
      "|7087500|      NULL| 9370.35|NULL|\n",
      "|   NULL|2024-11-24| 3679.19|2024|\n",
      "|7087500|2024-12-24|  7730.6|2024|\n",
      "|7087520|2020-02-19|14822.82|2020|\n",
      "|7087520|2020-03-20| 6668.04|2020|\n",
      "|7087520|2020-04-19|  5948.5|2020|\n",
      "|7087520|2020-05-19| 6760.61|2020|\n",
      "|7087520|2020-06-18|12243.51|2020|\n",
      "|7087520|2020-07-18| 3069.33|2020|\n",
      "+-------+----------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "req_df = req_df.withColumn(\"year\", year(\"date\"))\n",
    "print(req_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16970bbd-858f-4d72-925f-a299055adb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----+\n",
      "| emp_id|      date|  amount|year|\n",
      "+-------+----------+--------+----+\n",
      "|7087500|2024-09-25|    NULL|2024|\n",
      "|7087500|      NULL| 9370.35|NULL|\n",
      "|   NULL|2024-11-24| 3679.19|2024|\n",
      "|7087500|2024-12-24|  7730.6|2024|\n",
      "|7087520|2020-02-19|14822.82|2020|\n",
      "|7087520|2020-03-20| 6668.04|2020|\n",
      "|7087520|2020-04-19|  5948.5|2020|\n",
      "|7087520|2020-05-19| 6760.61|2020|\n",
      "|7087520|2020-06-18|12243.51|2020|\n",
      "|7087520|2020-07-18| 3069.33|2020|\n",
      "+-------+----------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col\n",
    "req_df = req_df.withColumn(\"year\", year(col(\"date\")))\n",
    "print(req_df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3da678db-0a98-4c71-8ecf-d2813bdfc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------+----+\n",
      "| emp_id|date|  amount|year|\n",
      "+-------+----+--------+----+\n",
      "|7087500|2024|    NULL|2024|\n",
      "|7087500|NULL| 9370.35|NULL|\n",
      "|   NULL|2024| 3679.19|2024|\n",
      "|7087500|2024|  7730.6|2024|\n",
      "|7087520|2020|14822.82|2020|\n",
      "|7087520|2020| 6668.04|2020|\n",
      "|7087520|2020|  5948.5|2020|\n",
      "|7087520|2020| 6760.61|2020|\n",
      "|7087520|2020|12243.51|2020|\n",
      "|7087520|2020| 3069.33|2020|\n",
      "+-------+----+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, col\n",
    "req_df.withColumn(\"date\", year(col(\"date\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "823bc1b8-3cad-49f8-82e0-c67ac49275a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c434dad-79f0-450d-b1b0-b10ca67644c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emp_id', 'date', 'amount', 'year']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3582a57-2879-4364-b6e7-74ddc778efe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+----+\n",
      "|emp_id|date|amount|year|\n",
      "+------+----+------+----+\n",
      "+------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.filter(col(\"date\")==\"2024\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8964bef7-b7d4-4cfc-b510-658779342432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_id', 'int'), ('date', 'date'), ('amount', 'double'), ('year', 'int')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe94fb0b-91c0-43fa-b9d4-61e6485ce01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----+\n",
      "| emp_id|      date|  amount|year|\n",
      "+-------+----------+--------+----+\n",
      "|7087500|2024-09-25|    NULL|2024|\n",
      "|7087500|      NULL| 9370.35|NULL|\n",
      "|   NULL|2024-11-24| 3679.19|2024|\n",
      "|7087500|2024-12-24|  7730.6|2024|\n",
      "|7087520|2020-02-19|14822.82|2020|\n",
      "|7087520|2020-03-20| 6668.04|2020|\n",
      "|7087520|2020-04-19|  5948.5|2020|\n",
      "|7087520|2020-05-19| 6760.61|2020|\n",
      "|7087520|2020-06-18|12243.51|2020|\n",
      "|7087520|2020-07-18| 3069.33|2020|\n",
      "+-------+----------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa336af3-ffb6-4d3a-a7dc-b0667a45724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+----+\n",
      "| emp_id|      date| amount|year|\n",
      "+-------+----------+-------+----+\n",
      "|7087500|2024-09-25|   NULL|2024|\n",
      "|   NULL|2024-11-24|3679.19|2024|\n",
      "|7087500|2024-12-24| 7730.6|2024|\n",
      "+-------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.filter(col(\"year\")==2024).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e5590e5-d56f-4e63-a588-75b4f2db261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "| emp_id|      date|amount|year|\n",
      "+-------+----------+------+----+\n",
      "|7087500|2024-09-25|  NULL|2024|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.filter((col(\"year\")==2024) & (col(\"amount\").isNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5760c78-97a6-45f0-8c22-97fb60096256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "print(dir(req_df.amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1316a6a-c3be-479e-9069-178c05189096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+----+\n",
      "| emp_id|      date| amount|year|\n",
      "+-------+----------+-------+----+\n",
      "|   NULL|2024-11-24|3679.19|2024|\n",
      "|7087500|2024-12-24| 7730.6|2024|\n",
      "+-------+----------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.filter((col(\"year\")==2024) & (col(\"amount\").isNotNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39764cc3-13fa-4c36-bbca-e4a926e76205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+----+\n",
      "| emp_id|      date|  amount|year|\n",
      "+-------+----------+--------+----+\n",
      "|   NULL|2024-11-24| 3679.19|2024|\n",
      "|7087500|2024-12-24|  7730.6|2024|\n",
      "|7087520|2021-01-14|11684.05|2021|\n",
      "|7087520|2021-02-13|13201.85|2021|\n",
      "|7087520|2021-03-15|  5000.9|2021|\n",
      "|7087520|2021-04-14| 7511.35|2021|\n",
      "|7087520|2021-05-14|11958.64|2021|\n",
      "|7087520|2021-06-13| 3903.62|2021|\n",
      "|7087520|2021-07-13| 5822.87|2021|\n",
      "|7087520|2021-08-12| 9035.41|2021|\n",
      "|7087520|2021-09-11| 5094.17|2021|\n",
      "|7087520|2021-10-11| 9397.57|2021|\n",
      "|7087520|2021-11-10|12942.52|2021|\n",
      "|7087520|2021-12-10| 4074.86|2021|\n",
      "+-------+----------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_df.filter((col(\"year\").isin(2024,2021)) & (col(\"amount\").isNotNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc640ee0-101d-42d5-9a04-0f21a54ef5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8036a78c-de3c-4a98-bc13-e374e7beb491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- salary_id: integer (nullable = true)\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- salary_month: date (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\salary50.txt\", header=True, inferSchema=True)\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d91025b-be0f-46ca-a5ea-a850c33f4a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8307d8-ead1-48f8-82f5-f36737b75e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63323d8-d5f5-424c-9c43-49335aaade54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#SQL DDL schema creation\n",
    "schema = \"id INT, name STRING, age FLOAT\"\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9c52ac-0412-4569-8608-bd622d94b598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#SQL DDL schema creation\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", FloatType(), True),\n",
    "    \n",
    "])\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8871ad4a-853d-46c8-bc20-8199ac6eed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|NULL|\n",
      "|  2|  Vamshi|NULL|\n",
      "|  3|Bhagavan|NULL|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#SQL DDL schema creation\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    \n",
    "])\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3090bdc-0911-4932-862b-727aae81c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|NULL|\n",
      "|  2|  Vamshi|NULL|\n",
      "|  3|Bhagavan|NULL|\n",
      "|  4|     sAM|  35|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    \n",
    "])\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02feafd8-c212-4f5c-820d-73cb3dde5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  4|     sAM|35.0|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", FloatType(), True),\n",
    "    \n",
    "])\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66b4d4d3-21cc-4e79-8ca1-1e587abcc1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99a0914-d63a-48be-9eaa-53a1964c683e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1.2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '1.2'"
     ]
    }
   ],
   "source": [
    "int(\"1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbee4e61-63c5-4b3b-a3a2-d0f2e1fd2c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n",
      "None\n",
      "+----+--------+----+\n",
      "|  id|    name| age|\n",
      "+----+--------+----+\n",
      "|   1|  Satish|28.2|\n",
      "|   2|  Vamshi|27.3|\n",
      "|   3|Bhagavan|29.1|\n",
      "|   4|     sAM|35.0|\n",
      "|NULL|     123|NULL|\n",
      "+----+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", FloatType(), True),\n",
    "    \n",
    "])\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a50657-bb12-416c-bd07-c1d517690a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e199119-0f1c-4a3d-adbd-d1cb2e132bbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '28.2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m28.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '28.2'"
     ]
    }
   ],
   "source": [
    "int(\"28.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11129385-891f-4414-bd46-3d4b29cdda5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+----+--------+----+\n",
      "|  id|    name| age|\n",
      "+----+--------+----+\n",
      "|   1|  Satish|NULL|\n",
      "|   2|  Vamshi|NULL|\n",
      "|   3|Bhagavan|NULL|\n",
      "|   4|     sAM|  35|\n",
      "|NULL|     123|NULL|\n",
      "+----+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    \n",
    "])\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, schema=schema)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a8dd62b-24d4-4682-8c98-c1da1c4308da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  4|     sAM|  35|\n",
      "|  5|     123|   a|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4036b7-bbba-44e3-9f36-77b31a3af2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  4|     sAM|35.0|\n",
      "+---+--------+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eca7fe6-e7fd-46ee-80e2-6092ca75b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "None\n",
      "+---+--------+-----+\n",
      "| id|    name|  age|\n",
      "+---+--------+-----+\n",
      "|  1|  Satish| 28.2|\n",
      "|  2|  Vamshi| 27.3|\n",
      "|  3|Bhagavan| 29.1|\n",
      "|  4|     sAM| 35.0|\n",
      "|  5|     123|34.56|\n",
      "+---+--------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "print(df.printSchema())\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "221e2348-5ff5-4d37-b23a-d45c222cc4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+-------+\n",
      "| id|    name|  age|age_int|\n",
      "+---+--------+-----+-------+\n",
      "|  1|  Satish| 28.2|     28|\n",
      "|  2|  Vamshi| 27.3|     27|\n",
      "|  3|Bhagavan| 29.1|     29|\n",
      "|  4|     sAM| 35.0|     35|\n",
      "|  5|     123|34.56|     34|\n",
      "+---+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"age_int\",col(\"age\").cast(IntegerType()))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d78af1d-443d-4fd1-80ea-4b21427503e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|    name|age|\n",
      "+---+--------+---+\n",
      "|  1|  Satish| 28|\n",
      "|  2|  Vamshi| 27|\n",
      "|  3|Bhagavan| 29|\n",
      "|  4|     sAM| 35|\n",
      "|  5|     123| 34|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"age\",col(\"age\").cast(IntegerType()))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e3f51f7-aba4-4096-ab22-dd5bd5edddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+--------+\n",
      "| id|    name|  age|name_int|\n",
      "+---+--------+-----+--------+\n",
      "|  1|  Satish| 28.2|    NULL|\n",
      "|  2|  Vamshi| 27.3|    NULL|\n",
      "|  3|Bhagavan| 29.1|    NULL|\n",
      "|  4|     sAM| 35.0|    NULL|\n",
      "|  5|     123|34.56|     123|\n",
      "+---+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"name_int\",col(\"name\").cast(IntegerType()))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb49de98-5e7a-40da-88e9-f3024c5321ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ed14502-92b6-4974-9aea-d549392035ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faa163d0-05a9-439a-b930-ef882f440b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, name: string, age: double]\n"
     ]
    }
   ],
   "source": [
    "#df.limit(3), transformation\n",
    "#df.show(3), action\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_3 = df.limit(3)\n",
    "print(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5a7cd24-d46e-4f90-a8f0-52e4d2a21edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#df.limit(3), transformation\n",
    "#df.show(3), action\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_3 = df.limit(3)\n",
    "print(df_3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea57d1dd-9985-46ae-a19e-e8f83bf1e71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "**********\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#df.limit(3), transformation\n",
    "#df.show(3), action\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_3 = df.show(3)\n",
    "print(\"*\"*10)\n",
    "print(df_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5195dd84-accb-4422-a66f-934725aeb2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    name|  age|\n",
      "+---+--------+-----+\n",
      "|  1|  Satish| 28.2|\n",
      "|  2|  Vamshi| 27.3|\n",
      "|  3|Bhagavan| 29.1|\n",
      "|  4|     sAM| 35.0|\n",
      "|  5|     123|34.56|\n",
      "+---+--------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "124e256a-6032-40ef-9fe8-c7c4c7d6be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    name|  age|\n",
      "+---+--------+-----+\n",
      "|  4|     sAM| 35.0|\n",
      "|  2|  Vamshi| 27.3|\n",
      "|  1|  Satish| 28.2|\n",
      "|  3|Bhagavan| 29.1|\n",
      "|  5|     123|34.56|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.sort(col(\"name\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "280fc2d9-2542-47b4-8b49-9eda581f9ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "49\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "print(ord(\"s\"))\n",
    "print(ord(\"1\"))\n",
    "print(ord(\"V\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bc9bc6c-a461-4444-b4bf-7334bffc012b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----+\n",
      "| id|    name|  age|\n",
      "+---+--------+-----+\n",
      "|  1|  Satish| 28.2|\n",
      "|  2|  Vamshi| 27.3|\n",
      "|  3|Bhagavan| 29.1|\n",
      "|  4|     sAM| 35.0|\n",
      "|  5|     123|34.56|\n",
      "+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe938d0c-bce4-4b1f-853e-741f06037b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  4|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  3|  Vamshi|28.3|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  1|  Satish|28.2|\n",
      "|  4|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.sort([col(\"name\"),col(\"age\")],ascending=[0,0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2290b362-2549-4dca-8848-c99fd6a5a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  4|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  1|  Satish|28.2|\n",
      "|  4|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.sort([col(\"name\"),col(\"age\")],ascending=[0,1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e9bdcf3-9f7c-4539-b089-854286e185bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  2|  Vamshi|27.3|\n",
      "+---+--------+----+\n",
      "\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  4|Bhagavan|29.1|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  1|  Satish|28.2|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75dc38fd-8788-49e7-aa8b-d5fe0c326aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  2|  Vamshi|27.3|\n",
      "+---+--------+----+\n",
      "\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  4|Bhagavan|29.1|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  1|  Satish|28.2|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74a6763b-57f2-4a84-a786-820fe9bf3b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|  Vamshi|28.3|\n",
      "|  4|Bhagavan|29.1|\n",
      "|  5|Bhagavan|29.1|\n",
      "|  6|Bhagavan|29.2|\n",
      "|  7|  Vamshi|27.3|\n",
      "|  8|  Vamshi|27.3|\n",
      "+---+--------+----+\n",
      "\n",
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  4|Bhagavan|29.1|\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.drop_duplicates(subset=[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac7bc7-b296-49d5-a7c5-fefad394b9cf",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf2d0d2-5dd0-4850-aee0-578b05a01d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86501fc3-1a4b-4cc3-bf6c-b487364c172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+\n",
      "| id|     name| age|\n",
      "+---+---------+----+\n",
      "|  1|   Satish|28.2|\n",
      "|  2|   Vamshi|27.3|\n",
      "|  3|   Vamshi|28.3|\n",
      "|  4| Bhagavan|29.1|\n",
      "|  5| Bhagavan|29.1|\n",
      "|  6| Bhagavan|29.2|\n",
      "|  7|   Vamshi|27.3|\n",
      "|  8|   Vamshi|27.3|\n",
      "|  1|    Ameel|28.2|\n",
      "|  2|  Shravan|27.3|\n",
      "|  3|Dhanunjay|29.1|\n",
      "+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_samecolumns = spark.read.csv(\"data\\\\sample_employee_data_same_columns.txt\", header=True, inferSchema=True)\n",
    "df_union = df.union(df_samecolumns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3012312-d950-4351-9334-e6b42b6f364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|    name|     age|\n",
      "+---+--------+--------+\n",
      "|  1|  Satish|    28.2|\n",
      "|  2|  Vamshi|    27.3|\n",
      "|  3|Bhagavan|    29.1|\n",
      "|  1|    28.2|  Satish|\n",
      "|  2|    27.3|  Vamshi|\n",
      "|  3|    29.1|Bhagavan|\n",
      "+---+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_samecolumns = spark.read.csv(\"data\\\\sample_employee_data_order_miss.txt\", header=True, inferSchema=True)\n",
    "df_union = df.union(df_samecolumns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e4d1e2-6397-4587-b8ab-8f511a6a7a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  1|     Sam|35.0|\n",
      "|  2|     jay| 7.0|\n",
      "|  3|   vedha| 6.0|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_order_miss.txt\", header=True, inferSchema=True)\n",
    "df_union = df.unionByName(df_missmatched_columns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45be488-0405-48bb-a999-42f61f1ee205",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 4 columns.;\n'Union false, false\n:- Relation [id#572,name#573,age#574] csv\n+- Project [id#595, name#596, age#597, phone#598L]\n   +- Relation [id#595,name#596,age#597,phone#598L] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample_employee_data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m df_missmatched_columns \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample_employee_data_extra_col.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m df_union \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39munionByName(df_missmatched_columns)\n\u001b[0;32m      4\u001b[0m df_union\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:4043\u001b[0m, in \u001b[0;36mDataFrame.unionByName\u001b[1;34m(self, other, allowMissingColumns)\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munionByName\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, allowMissingColumns: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3966\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3968\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4041\u001b[0m \u001b[38;5;124;03m    +----+----+----+----+----+----+\u001b[39;00m\n\u001b[0;32m   4042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39munionByName(other\u001b[38;5;241m.\u001b[39m_jdf, allowMissingColumns), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 4 columns.;\n'Union false, false\n:- Relation [id#572,name#573,age#574] csv\n+- Project [id#595, name#596, age#597, phone#598L]\n   +- Relation [id#595,name#596,age#597,phone#598L] csv\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df_union = df.unionByName(df_missmatched_columns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910bff40-6b54-4711-a718-e9f103583fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 4 columns.;\n'Union false, false\n:- Relation [id#621,name#622,age#623] csv\n+- Relation [id#644,name#645,age#646,phone#647L] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample_employee_data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m df_missmatched_columns \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample_employee_data_extra_col.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m df_union \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39munion(df_missmatched_columns)\n\u001b[0;32m      4\u001b[0m df_union\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3929\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   3833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munion\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3834\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing the union of rows in this and another\u001b[39;00m\n\u001b[0;32m   3835\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3836\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3927\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[0;32m   3928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39munion(other\u001b[38;5;241m.\u001b[39m_jdf), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 3 columns and the second input has 4 columns.;\n'Union false, false\n:- Relation [id#621,name#622,age#623] csv\n+- Relation [id#644,name#645,age#646,phone#647L] csv\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df_union = df.union(df_missmatched_columns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbaf6a92-f20d-4981-9835-552294d293bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"age\" among (id, name, phone).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample_employee_data.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m df_missmatched_columns \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msample_employee_data_extra_col.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m df_union \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39munionByName(df_missmatched_columns)\n\u001b[0;32m      4\u001b[0m df_union\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:4043\u001b[0m, in \u001b[0;36mDataFrame.unionByName\u001b[1;34m(self, other, allowMissingColumns)\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munionByName\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, allowMissingColumns: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3966\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3968\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4041\u001b[0m \u001b[38;5;124;03m    +----+----+----+----+----+----+\u001b[39;00m\n\u001b[0;32m   4042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39munionByName(other\u001b[38;5;241m.\u001b[39m_jdf, allowMissingColumns), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \"age\" among (id, name, phone)."
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df_union = df.unionByName(df_missmatched_columns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348e3189-5060-45a0-bb2d-eea2a15bf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|    name|          age|\n",
      "+---+--------+-------------+\n",
      "|  1|  Satish|         28.2|\n",
      "|  2|  Vamshi|         27.3|\n",
      "|  3|Bhagavan|         29.1|\n",
      "|  1|     sam|9.786567845E9|\n",
      "|  2|     jay|9.786567846E9|\n",
      "|  3|   vedha|9.786567847E9|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df_union = df.union(df_missmatched_columns)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3c13f7-a556-4c61-a2b1-645bcf6cf89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munionByName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowMissingColumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` containing union of rows in this and another\n",
       ":class:`DataFrame`.\n",
       "\n",
       "This method performs a union operation on both input DataFrames, resolving columns by\n",
       "name (rather than position). When `allowMissingColumns` is True, missing columns will\n",
       "be filled with null.\n",
       "\n",
       ".. versionadded:: 2.3.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "other : :class:`DataFrame`\n",
       "    Another :class:`DataFrame` that needs to be combined.\n",
       "allowMissingColumns : bool, optional, default False\n",
       "   Specify whether to allow missing columns.\n",
       "\n",
       "   .. versionadded:: 3.1.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    A new :class:`DataFrame` containing the combined rows with corresponding\n",
       "    columns of the two given DataFrames.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Example 1: Union of two DataFrames with same columns in different order.\n",
       "\n",
       ">>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
       ">>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
       ">>> df1.unionByName(df2).show()\n",
       "+----+----+----+\n",
       "|col0|col1|col2|\n",
       "+----+----+----+\n",
       "|   1|   2|   3|\n",
       "|   6|   4|   5|\n",
       "+----+----+----+\n",
       "\n",
       "Example 2: Union with missing columns and setting `allowMissingColumns=True`.\n",
       "\n",
       ">>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
       ">>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
       ">>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
       "+----+----+----+----+\n",
       "|col0|col1|col2|col3|\n",
       "+----+----+----+----+\n",
       "|   1|   2|   3|NULL|\n",
       "|NULL|   4|   5|   6|\n",
       "+----+----+----+----+\n",
       "\n",
       "Example 3: Union of two DataFrames with few common columns.\n",
       "\n",
       ">>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
       ">>> df2 = spark.createDataFrame([[4, 5, 6, 7]], [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
       ">>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
       "+----+----+----+----+----+\n",
       "|col0|col1|col2|col3|col4|\n",
       "+----+----+----+----+----+\n",
       "|   1|   2|   3|NULL|NULL|\n",
       "|NULL|   4|   5|   6|   7|\n",
       "+----+----+----+----+----+\n",
       "\n",
       "Example 4: Union of two DataFrames with completely different columns.\n",
       "\n",
       ">>> df1 = spark.createDataFrame([[0, 1, 2]], [\"col0\", \"col1\", \"col2\"])\n",
       ">>> df2 = spark.createDataFrame([[3, 4, 5]], [\"col3\", \"col4\", \"col5\"])\n",
       ">>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
       "+----+----+----+----+----+----+\n",
       "|col0|col1|col2|col3|col4|col5|\n",
       "+----+----+----+----+----+----+\n",
       "|   0|   1|   2|NULL|NULL|NULL|\n",
       "|NULL|NULL|NULL|   3|   4|   5|\n",
       "+----+----+----+----+----+----+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df_union = df.unionByName?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b01209-f1b3-4998-8bde-ce32078cf22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|  Satish|\n",
      "|  2|  Vamshi|\n",
      "|  3|Bhagavan|\n",
      "|  1|     sam|\n",
      "|  2|     jay|\n",
      "|  3|   vedha|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df.select([\"id\", \"name\"]).unionByName(df_missmatched_columns.select([\"id\",\"name\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7d28ea-5636-4ab2-a88e-8da704ba2737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+----------+\n",
      "| id|    name| age|     phone|\n",
      "+---+--------+----+----------+\n",
      "|  1|  Satish|28.2|      NULL|\n",
      "|  2|  Vamshi|27.3|      NULL|\n",
      "|  3|Bhagavan|29.1|      NULL|\n",
      "|  1|     sam|NULL|9786567845|\n",
      "|  2|     jay|NULL|9786567846|\n",
      "|  3|   vedha|NULL|9786567847|\n",
      "+---+--------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_extra_col.txt\", header=True, inferSchema=True)\n",
    "df_union = df.unionByName(df_missmatched_columns, allowMissingColumns=True)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b611f3a2-c3f6-43a7-83ff-e85130159611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_same_columns.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns.printSchema()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f44e896f-da4c-4d7e-8fce-bc25f260d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "+----+---------+----+\n",
      "|  id|     name| age|\n",
      "+----+---------+----+\n",
      "|   1|   Satish|28.2|\n",
      "|   2|   Vamshi|27.3|\n",
      "|   3| Bhagavan|29.1|\n",
      "|1a23|    Ameel|28.2|\n",
      "|2bc4|  Shravan|27.3|\n",
      "|3de5|Dhanunjay|29.1|\n",
      "+----+---------+----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_same_columns.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns.printSchema()\n",
    "df.printSchema()\n",
    "df_u = df.union(df_missmatched_columns)\n",
    "df_u.show()\n",
    "df_u.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7115b0c-3851-4ea3-b232-f9ab9080c3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "+---+---------+----+\n",
      "| id|     name| age|\n",
      "+---+---------+----+\n",
      "|1.0|   Satish|28.2|\n",
      "|2.0|   Vamshi|27.3|\n",
      "|3.0| Bhagavan|29.1|\n",
      "|2.3|    Ameel|28.2|\n",
      "|4.5|  Shravan|27.3|\n",
      "|6.7|Dhanunjay|29.1|\n",
      "+---+---------+----+\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data\\\\sample_employee_data.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns = spark.read.csv(\"data\\\\sample_employee_data_same_columns.txt\", header=True, inferSchema=True)\n",
    "df_missmatched_columns.printSchema()\n",
    "df.printSchema()\n",
    "df_u = df.union(df_missmatched_columns)\n",
    "df_u.show()\n",
    "df_u.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb054686-2cfd-4b19-8c1e-c99f9f30f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
