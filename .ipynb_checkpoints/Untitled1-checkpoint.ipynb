{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d20939-2d83-4299-9437-e78aa68cad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155a197b-c00f-408d-af12-82ce32e0b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfbfc3-7873-4a6e-aac3-bac7dcb2da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18b3c76-d0fe-4834-a607-693b21abef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a9dcb3-a703-44fc-9622-cf2aa3fb57a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m4.\u001b[39m,\u001b[38;5;241m5.\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring3\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m: [date(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), date(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), date(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m: [datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m), datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m), datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m      7\u001b[0m })\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(pandas_df)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3,4],\n",
    "    'b': [2., 3., 4.,5.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551b7fc1-f225-462e-ae9b-5b5af7fe4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|5.0|   NULL|      NULL|               NULL|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': pd.Series([1, 2, 3,4]),\n",
    "    'b': pd.Series([2., 3., 4.,5.]),\n",
    "    'c': pd.Series(['string1', 'string2', 'string3']),\n",
    "    'd': pd.Series([date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)]),\n",
    "    'e': pd.Series([datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)])\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8206010b-9aed-43cf-9439-93e0b96b023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a    b        c           d                   e\n",
      "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
      "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
      "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00\n",
      "3  4  5.0      NaN         NaN                 NaT\n"
     ]
    }
   ],
   "source": [
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4687afb-f8ab-4864-a611-49c61f5c2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+----+-------+----------+-------------------+\n",
      "|  a|   b|      c|         d|                  e|\n",
      "+---+----+-------+----------+-------------------+\n",
      "|  1| 2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2| 3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3| 4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|NULL|   NULL|      NULL|               NULL|\n",
      "+---+----+-------+----------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': pd.Series([1, 2, 3,4]),\n",
    "    'b': pd.Series([2., 3., 4.]),\n",
    "    'c': pd.Series(['string1', 'string2', 'string3']),\n",
    "    'd': pd.Series([date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)]),\n",
    "    'e': pd.Series([datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)])\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62f6b39-d855-4f46-bf20-211ee45160d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'b'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb3a8ac-4a29-4734-b4f2-ca70efd7c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "print(dir(df.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed98a124-bd11-4fb9-a10f-463fbc3513e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[b: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac358a26-1680-4fce-bab2-ba965fb99bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'agg', 'alias', 'approxQuantile', 'b', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n",
      "********************\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "df_b = df.select(\"b\")\n",
    "df_col_b = df.b\n",
    "print(dir(df_b))\n",
    "print(\"*\"*20)\n",
    "print(dir(df_col_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16345bbc-08e6-49e3-9f8d-31df71844e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"b\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b802233a-17d4-4493-828c-3bc5f1fb1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d242880-9917-483c-a796-3f0b66a399ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d293a83-9c89-4353-a152-1f6e2bc830b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93470b0c-043b-4c3e-8daf-578fd43f32b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5974f3a8-74e4-47d6-a5c0-9066ba2ee4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf9dae0-214c-4eb4-a24b-056f378973e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "     a\n",
      "0  1.0\n",
      "1  2.0\n",
      "2  NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "343fe5dc-8d2a-459e-abeb-b0ce13fd91cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b84b62c-be1e-4ec0-9041-648196eb027f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    2.0\n",
       "2    NaN\n",
       "Name: a, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f5e7435-ab12-436e-a65e-fb08ed5bd6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df1.a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3982e36-eac4-4f87-8d34-e29b124b678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0df53d17-1e98-4288-88f5-2a4e2cc67a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "+----+\n",
      "|   a|\n",
      "+----+\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "|NULL|\n",
      "+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c5f9ca1-e1ae-4b91-9ddf-efb0a8313a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "-RECORD 0---\n",
      " a   | 1.0  \n",
      "-RECORD 1---\n",
      " a   | 2.0  \n",
      "-RECORD 2---\n",
      " a   | NULL \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "print(sdf.show(vertical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51d546eb-6531-416c-8cb3-bcbf67a4a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "-RECORD 0---\n",
      " a   | 1.0  \n",
      "-RECORD 1---\n",
      " a   | 2.0  \n",
      "-RECORD 2---\n",
      " a   | NULL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "sdf.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "958f9d19-371f-40aa-8593-b951b1daa455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "-RECORD 0--\n",
      " a   | 1.0 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "sdf.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57e8e230-d274-46dd-83e4-9f783310bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double, b: double]\n",
      "-RECORD 0--\n",
      " a   | 1.0 \n",
      " b   | 1.0 \n",
      "-RECORD 1--\n",
      " a   | 2.0 \n",
      " b   | 2.0 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "sdf.show(2,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac794e14-a74c-4be4-bed9-af04eafe5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "| 1.0| 1.0|\n",
      "| 2.0| 2.0|\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fd5b93f-86dd-4a55-ad88-7db8712f903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "| 1.0| 1.0|\n",
      "| 2.0| 2.0|\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
       "\n",
       ".. versionadded:: 3.0.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "func : function\n",
       "    a function that takes and returns a :class:`DataFrame`.\n",
       "*args\n",
       "    Positional arguments to pass to func.\n",
       "\n",
       "    .. versionadded:: 3.3.0\n",
       "**kwargs\n",
       "    Keyword arguments to pass to func.\n",
       "\n",
       "    .. versionadded:: 3.3.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    Transformed DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql.functions import col\n",
       ">>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
       ">>> def cast_all_to_int(input_df):\n",
       "...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
       "...\n",
       ">>> def sort_columns_asc(input_df):\n",
       "...     return input_df.select(*sorted(input_df.columns))\n",
       "...\n",
       ">>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
       "+-----+---+\n",
       "|float|int|\n",
       "+-----+---+\n",
       "|    1|  1|\n",
       "|    2|  2|\n",
       "+-----+---+\n",
       "\n",
       ">>> def add_n(input_df, n):\n",
       "...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
       "...                             for col_name in input_df.columns])\n",
       ">>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
       "+---+-----+\n",
       "|int|float|\n",
       "+---+-----+\n",
       "| 12| 12.0|\n",
       "| 13| 13.0|\n",
       "+---+-----+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.show())\n",
    "sdf.transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a9ebd-097d-4441-8086-4b549500d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4851bd5-8ec1-454d-950c-f9a02e1857dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function col at 0x0000025A6BA04400>\n",
      "Column<'col1'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "print(col)\n",
    "print(col(\"col1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "216093b4-8843-453a-83af-3fd66bf747d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "| 1.0| 1.0|\n",
      "| 2.0| 2.0|\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n",
      "None\n",
      "converted\n",
      "DataFrame[a: double, b: double]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[a: double, b: double]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.show())\n",
    "def convert(data):\n",
    "    print(\"converted\")\n",
    "    print(data)\n",
    "    return data\n",
    "sdf.transform(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e8bb327-e55a-4654-aaac-deed3995e1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted\n",
      "DataFrame[a: double, b: double]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[a: double, b: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "def convert(data):\n",
    "    print(\"converted\")\n",
    "    print(data)\n",
    "    return data\n",
    "sdf.transform(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c5e2d6-e911-44b7-952c-36d108a5e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ec4fa9-73a6-42ff-abde-867a7f073d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  2| 2.0|\n",
      "|  3|NULL|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1b41ec-1419-4025-b9b5-3493bfdc8c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fd3f67-8561-44c8-950a-08ce4d3f3f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'CAST(a AS FLOAT)'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.a.cast(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cce006-fad2-400c-80f9-109a2c285a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e50329a-1dee-46a2-ac08-2d77f1bd72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a.cast(\"double\")\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac2fcf0-5163-4462-9877-049b578df2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"double\")\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9729d7db-d042-4cdb-84b9-62f7554a62a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6fd2751-5aff-4331-b8de-baf57e793fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'a', 'agg', 'alias', 'approxQuantile', 'b', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
     ]
    }
   ],
   "source": [
    "print(dir(sdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674690e7-4c9f-46ee-89ee-f9bb278f37b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n",
      "DataFrame[a: bigint, b: double]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)\n",
    "sdf.drop(\"a\")\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b270741f-5ae7-47dd-998a-d5bcd60456ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n",
      "DataFrame[a: bigint, b: double]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ColumnOrName'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` without specified columns.\n",
       "This is a no-op if the schema doesn't contain the given column name(s).\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "cols: str or :class:`Column`\n",
       "    a name of the column, or the :class:`Column` to drop\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    DataFrame without given columns.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When an input is a column name, it is treated literally without further interpretation.\n",
       "Otherwise, will try to match the equivalent expression.\n",
       "So that dropping column by its name `drop(colName)` has different semantic with directly\n",
       "dropping the column `drop(col(colName))`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql import Row\n",
       ">>> from pyspark.sql.functions import col, lit\n",
       ">>> df = spark.createDataFrame(\n",
       "...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
       ">>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
       "\n",
       ">>> df.drop('age').show()\n",
       "+-----+\n",
       "| name|\n",
       "+-----+\n",
       "|  Tom|\n",
       "|Alice|\n",
       "|  Bob|\n",
       "+-----+\n",
       ">>> df.drop(df.age).show()\n",
       "+-----+\n",
       "| name|\n",
       "+-----+\n",
       "|  Tom|\n",
       "|Alice|\n",
       "|  Bob|\n",
       "+-----+\n",
       "\n",
       "Drop the column that joined both DataFrames on.\n",
       "\n",
       ">>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
       "+---+------+\n",
       "|age|height|\n",
       "+---+------+\n",
       "| 14|    80|\n",
       "| 16|    85|\n",
       "+---+------+\n",
       "\n",
       ">>> df3 = df.join(df2)\n",
       ">>> df3.show()\n",
       "+---+-----+------+----+\n",
       "|age| name|height|name|\n",
       "+---+-----+------+----+\n",
       "| 14|  Tom|    80| Tom|\n",
       "| 14|  Tom|    85| Bob|\n",
       "| 23|Alice|    80| Tom|\n",
       "| 23|Alice|    85| Bob|\n",
       "| 16|  Bob|    80| Tom|\n",
       "| 16|  Bob|    85| Bob|\n",
       "+---+-----+------+----+\n",
       "\n",
       "Drop two column by the same name.\n",
       "\n",
       ">>> df3.drop(\"name\").show()\n",
       "+---+------+\n",
       "|age|height|\n",
       "+---+------+\n",
       "| 14|    80|\n",
       "| 14|    85|\n",
       "| 23|    80|\n",
       "| 23|    85|\n",
       "| 16|    80|\n",
       "| 16|    85|\n",
       "+---+------+\n",
       "\n",
       "Can not drop col('name') due to ambiguous reference.\n",
       "\n",
       ">>> df3.drop(col(\"name\")).show()\n",
       "Traceback (most recent call last):\n",
       "...\n",
       "pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
       "\n",
       ">>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
       ">>> df4.show()\n",
       "+---+-----+-----+\n",
       "|age| name|a.b.c|\n",
       "+---+-----+-----+\n",
       "| 14|  Tom|    1|\n",
       "| 23|Alice|    1|\n",
       "| 16|  Bob|    1|\n",
       "+---+-----+-----+\n",
       "\n",
       ">>> df4.drop(\"a.b.c\").show()\n",
       "+---+-----+\n",
       "|age| name|\n",
       "+---+-----+\n",
       "| 14|  Tom|\n",
       "| 23|Alice|\n",
       "| 16|  Bob|\n",
       "+---+-----+\n",
       "\n",
       "Can not find a column matching the expression \"a.b.c\".\n",
       "\n",
       ">>> df4.drop(col(\"a.b.c\")).show()\n",
       "+---+-----+-----+\n",
       "|age| name|a.b.c|\n",
       "+---+-----+-----+\n",
       "| 14|  Tom|    1|\n",
       "| 23|Alice|    1|\n",
       "| 16|  Bob|    1|\n",
       "+---+-----+-----+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)\n",
    "sdf.drop?\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5228faa8-5027-4a33-8250-f5a07e4fadff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n",
      "+----+\n",
      "|   b|\n",
      "+----+\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "|NULL|\n",
      "+----+\n",
      "\n",
      "DataFrame[a: bigint, b: double]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)\n",
    "sdf.drop(\"a\").show()\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5557bcd-8f92-4ee5-9f96-8744ef52e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  2| 2.0|\n",
      "|  3|NULL|\n",
      "+---+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "sdf.c=pd.Series([1,2,3])\n",
    "print(sdf.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4e181e-ef15-4116-bb0d-b6adfd26283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  2| 2.0|\n",
      "|  3|NULL|\n",
      "+---+----+\n",
      "\n",
      "None\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'a', 'agg', 'alias', 'approxQuantile', 'b', 'c', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "sdf.c=pd.Series([1,2,3])\n",
    "print(sdf.show())\n",
    "print(dir(sdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937909b3-a820-48f1-9d8e-bf385f5c3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eb04ca-4bd8-4b3f-97fd-e6ff61ae5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477845e3-8096-4582-a7ad-1ba74fd8924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JobTracking\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d0b0c3-59a0-4ee8-8de6-7efc75ec57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebaa95b-ba07-45f8-8ce7-07ed496dc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JobTracking\") \\\n",
    "    .config(\"spark.ui.port\", \"4041\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68b2e86-81ea-49ec-a3aa-9601ec27cbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Navya\\AppData\\Local\\Temp\\ipykernel_6684\\2161328399.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df_1cr = spark.read.csv(\"data\\salary_1cr.csv\", inferSchema=True, header=True)\n"
     ]
    }
   ],
   "source": [
    "df_1cr = spark.read.csv(\"data\\salary_1cr.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba7ab58-08f4-4efd-a7db-b21172e40c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[salary_id: int, emp_id: int, salary_month: date, amount: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e02b5f-28ec-41da-b77e-492e73f67aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1cr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0cbe7d-cac5-4e87-b68a-a3f9ef5d49f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary_id', 'emp_id', 'salary_month', 'amount']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1cr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "489f9b9c-77c4-4354-895c-9b2d15fc4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|  emp_id|salary_month|\n",
      "+--------+------------+\n",
      "|39470031|  2020-10-16|\n",
      "|39470031|  2020-11-15|\n",
      "+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a one more df with some columns\n",
    "df_emp_id_amount = df_1cr.select(\"emp_id\",\"salary_month\")\n",
    "df_emp_id_amount.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf3d95-3f8d-4de9-b4f0-9926664d4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a one more df with some columns\n",
    "df_emp_id_amount = df_1cr.select(\"emp_id\",\"salary_month\")\n",
    "print(dir(df_emp_id_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546866c6-4fa7-4caa-88fd-9bb0a2cfbd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|  emp_id|salary_month|\n",
      "+--------+------------+\n",
      "|39470031|  2020-10-16|\n",
      "|39470031|  2020-11-15|\n",
      "|39470031|  2020-12-15|\n",
      "+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the standardise way pyspark sugesting is using col function\n",
    "from pyspark.sql.functions import col\n",
    "df_emp_id_amount = df_1cr.select(col(\"emp_id\"),col(\"salary_month\"))\n",
    "print(df_emp_id_amount.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a945673-93fe-4675-94bf-4f35a5124f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|  emp_id|     month|\n",
      "+--------+----------+\n",
      "|39470031|2020-10-16|\n",
      "|39470031|2020-11-15|\n",
      "|39470031|2020-12-15|\n",
      "+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ALIAS: change the column name if you dont like\n",
    "df_emp_id_amount = df_1cr.select(col(\"emp_id\"),col(\"salary_month\").alias('month'))\n",
    "print(df_emp_id_amount.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac32c4ce-70c3-4855-8fed-b6993614dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "#Filter\n",
    "df_39470031 = df_1cr.filter(col(\"emp_id\")==39470031)\n",
    "print(df_39470031.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af55b76f-f6e6-4443-8e35-6b074669a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|  4504017|7087500|  2024-09-25|  NULL|\n",
      "+---------+-------+------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Filter 7087500 employee january month results\n",
    "from pyspark.sql.functions import col, month\n",
    "df_sal50 = spark.read.csv('data\\\\salary50.txt',header=True, inferSchema=True)\n",
    "df_7087500 = df_sal50.filter((col(\"emp_id\")==7087500) & (month(col(\"salary_month\"))==9))\n",
    "print(df_7087500.count())\n",
    "print(df_7087500.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2969e3-108a-436c-996d-57a382d156f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|  4504017|7087500|  2024-09-25|  NULL|\n",
      "|     NULL|7087500|  2024-12-24|7730.6|\n",
      "+---------+-------+------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Filter 7087500 employee january month results\n",
    "#isin?...  9,12\n",
    "from pyspark.sql.functions import col, month\n",
    "df_sal50 = spark.read.csv('data\\\\salary50.txt',header=True, inferSchema=True)\n",
    "df_7087500 = df_sal50.filter((col(\"emp_id\")==7087500) & (month(col(\"salary_month\")).isin(9,12)))\n",
    "print(df_7087500.count())\n",
    "print(df_7087500.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d35b4cd3-9cee-4ffb-bdcb-47febfbaffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|     NULL|7087500|  2024-12-24|7730.6|\n",
      "+---------+-------+------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Filter 7087500 employee january month results\n",
    "#isin?...  9,12\n",
    "from pyspark.sql.functions import col, month\n",
    "df_sal50 = spark.read.csv('data\\\\salary50.txt',header=True, inferSchema=True)\n",
    "df_7087500 = df_sal50.filter((col(\"emp_id\")==7087500) & (month(col(\"salary_month\")).isin(9,12)) & col('salary_id').isNull())\n",
    "print(df_7087500.count())\n",
    "print(df_7087500.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc123bf-5add-4730-b080-be4e27201abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1cr = spark.read.csv(\"data\\\\salary_1cr.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fd6bc56-1db0-4855-b0c6-97ba8db5cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[salary_id: int, emp_id: int, salary_month: date, amount: double]\n"
     ]
    }
   ],
   "source": [
    "# rename the columns\n",
    "print(df_1cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52bd2df8-1766-47c6-b226-0fdc2d5602d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[salary_id: int, emp_id: int, salary_month: date, amount: double]\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('salary_id',\"sal_id\")\n",
    "print(df_1cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cbba348-5063-4e0d-93bc-ec8daa8ec2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary_id', 'emp_id', 'salary_month', 'amount']\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('salary_id',\"sal_id\")\n",
    "print(df_1cr.columns) # dolumns are not actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fc744a6-9888-4444-af46-64ad2c6926de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-------+\n",
      "|salary_id|  emp_id|salary_month| amount|\n",
      "+---------+--------+------------+-------+\n",
      "|  9505409|39470031|  2020-10-16|7710.72|\n",
      "|  9505410|39470031|  2020-11-15|3114.26|\n",
      "|  9505411|39470031|  2020-12-15|10102.4|\n",
      "+---------+--------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('salary_id',\"sal_id\")\n",
    "df_1cr.show(3) # show is an actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43b9b6c2-4139-4052-8e86-fb55c5a1cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-------+\n",
      "|salary_id|  emp_id|salary_month| amount|\n",
      "+---------+--------+------------+-------+\n",
      "|  9505409|39470031|  2020-10-16|7710.72|\n",
      "|  9505410|39470031|  2020-11-15|3114.26|\n",
      "|  9505411|39470031|  2020-12-15|10102.4|\n",
      "+---------+--------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('emp_id',\"empid\")\n",
    "df_1cr.show(3) # show is an actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafdf69-0f2f-46ed-8684-3e8a78a71cc3",
   "metadata": {},
   "source": [
    "## the transformations will not change the existing dataframe. The SPRAK DATAFRAME IS IMMUTABLE OBJECT TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "213fa3df-ba96-4600-8cc8-a83093766a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-------+\n",
      "|salary_id|   empid|salary_month| amount|\n",
      "+---------+--------+------------+-------+\n",
      "|  9505409|39470031|  2020-10-16|7710.72|\n",
      "|  9505410|39470031|  2020-11-15|3114.26|\n",
      "|  9505411|39470031|  2020-12-15|10102.4|\n",
      "+---------+--------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1cr_renamed = df_1cr.withColumnRenamed('emp_id',\"empid\")\n",
    "df_1cr_renamed.show(3) # show is an actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79e63c-0e0f-47e9-a9a4-2d21c0a9200e",
   "metadata": {},
   "source": [
    "## ALL TRNASFORMATIONS RETURNS THE NEW DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b5631-c174-45aa-b388-e94b5043a071",
   "metadata": {},
   "source": [
    "### withColumn\n",
    "#### helps us to create new column or change the existing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0698dc3-b03b-4b97-805b-86f3f57ecd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4198691d-409a-45f7-a66a-e2354b55b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c502d6-dd57-4f54-81bb-f854957d1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data\\\\salary_1cr.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30d4022-d641-4a57-9f51-a2f14d767371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary_id', 'emp_id', 'salary_month', 'amount']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3423d3-2fa5-4330-a6ba-5e1670977e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7de159a-f21c-4767-bd6e-3203215367be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"YEAR\",year(df.salary_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ef191b-8e08-41bb-8ffe-2c8f048e1114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary_id', 'emp_id', 'salary_month', 'amount', 'YEAR']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf6e1e5-78c6-43ab-ac93-497b85d20c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------+----+\n",
      "|salary_id|  emp_id|salary_month|  amount|YEAR|\n",
      "+---------+--------+------------+--------+----+\n",
      "|  9505409|39470031|  2020-10-16| 7710.72|2020|\n",
      "|  9505410|39470031|  2020-11-15| 3114.26|2020|\n",
      "|  9505411|39470031|  2020-12-15| 10102.4|2020|\n",
      "|  9505412|39470031|  2021-01-14|13129.35|2020|\n",
      "|  9505413|39470031|  2021-02-13|  5642.5|2020|\n",
      "|  9505414|39470031|  2021-03-15|  7808.9|2020|\n",
      "|  9505415|39470031|  2021-04-14|  5634.8|2020|\n",
      "|  9505416|39470031|  2021-05-14|  9629.5|2020|\n",
      "|  9505417|39470031|  2021-06-13| 14030.1|2020|\n",
      "|  9505418|39470031|  2021-07-13|  6550.5|2020|\n",
      "+---------+--------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81947c8d-c90a-4fea-b968-3157975ac1ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got str.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5172\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5130\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[0;32m   5131\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5169\u001b[0m \u001b[38;5;124;03m+---+-----+----+\u001b[39;00m\n\u001b[0;32m   5170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m-> 5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[0;32m   5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_COLUMN] Argument `col` should be a Column, got str."
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"YEAR\",\"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e036fd0-34ae-4e64-85ee-9a30225e2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------+----+\n",
      "|salary_id|  emp_id|salary_month|  amount|YEAR|\n",
      "+---------+--------+------------+--------+----+\n",
      "|  9505409|39470031|  2020-10-16| 7710.72|2020|\n",
      "|  9505410|39470031|  2020-11-15| 3114.26|2020|\n",
      "|  9505411|39470031|  2020-12-15| 10102.4|2020|\n",
      "|  9505412|39470031|  2021-01-14|13129.35|2020|\n",
      "|  9505413|39470031|  2021-02-13|  5642.5|2020|\n",
      "|  9505414|39470031|  2021-03-15|  7808.9|2020|\n",
      "|  9505415|39470031|  2021-04-14|  5634.8|2020|\n",
      "|  9505416|39470031|  2021-05-14|  9629.5|2020|\n",
      "|  9505417|39470031|  2021-06-13| 14030.1|2020|\n",
      "|  9505418|39470031|  2021-07-13|  6550.5|2020|\n",
      "+---------+--------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = df.withColumn(\"YEAR\",lit(\"2020\"))\n",
    "print(df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "951d2bc2-506b-4ee9-b400-3baef0a2a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------+----+\n",
      "|salary_id|  emp_id|salary_month|  amount|YEAR|\n",
      "+---------+--------+------------+--------+----+\n",
      "|  9505409|39470031|  2020-10-16| 7710.72|NULL|\n",
      "|  9505410|39470031|  2020-11-15| 3114.26|NULL|\n",
      "|  9505411|39470031|  2020-12-15| 10102.4|NULL|\n",
      "|  9505412|39470031|  2021-01-14|13129.35|NULL|\n",
      "|  9505413|39470031|  2021-02-13|  5642.5|NULL|\n",
      "|  9505414|39470031|  2021-03-15|  7808.9|NULL|\n",
      "|  9505415|39470031|  2021-04-14|  5634.8|NULL|\n",
      "|  9505416|39470031|  2021-05-14|  9629.5|NULL|\n",
      "|  9505417|39470031|  2021-06-13| 14030.1|NULL|\n",
      "|  9505418|39470031|  2021-07-13|  6550.5|NULL|\n",
      "+---------+--------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = df.withColumn(\"YEAR\",lit(None))\n",
    "print(df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "390c66fe-7b1d-445b-a4a7-b85718890176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+\n",
      "|salary_id| emp_id|salary_month|  amount|\n",
      "+---------+-------+------------+--------+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|\n",
      "|  4504018|7087500|        NULL| 9370.35|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|\n",
      "|  4504022|7087520|  2020-03-20| 6668.04|\n",
      "|  4504023|7087520|  2020-04-19|  5948.5|\n",
      "|  4504024|7087520|  2020-05-19| 6760.61|\n",
      "|  4504025|7087520|  2020-06-18|12243.51|\n",
      "|  4504026|7087520|  2020-07-18| 3069.33|\n",
      "+---------+-------+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# doe the withColumn handles the NULL values?\n",
    "df_50null = spark.read.csv(\"data\\\\salary50.txt\", header=True, inferSchema=True)\n",
    "df_50null.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62a82d63-4d00-4328-9987-df45d52fe5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+----+\n",
      "|salary_id| emp_id|salary_month|  amount|YEAR|\n",
      "+---------+-------+------------+--------+----+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|2024|\n",
      "|  4504018|7087500|        NULL| 9370.35|NULL|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|2024|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|2024|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|2020|\n",
      "|  4504022|7087520|  2020-03-20| 6668.04|2020|\n",
      "|  4504023|7087520|  2020-04-19|  5948.5|2020|\n",
      "|  4504024|7087520|  2020-05-19| 6760.61|2020|\n",
      "|  4504025|7087520|  2020-06-18|12243.51|2020|\n",
      "|  4504026|7087520|  2020-07-18| 3069.33|2020|\n",
      "+---------+-------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_50null = df_50null.withColumn(\"YEAR\", year(col('salary_month')))\n",
    "print(df_50null.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6e0b4a-c61d-4781-9d6e-f97ce46080a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa07614-1f96-4415-94dc-304f47a18df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+\n",
      "|salary_id| emp_id|salary_month|  amount|\n",
      "+---------+-------+------------+--------+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|\n",
      "|  4504018|7087500|        NULL| 9370.35|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|\n",
      "+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data\\\\salary50.txt', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "319ba65c-a38d-4962-a169-b2a7222e02e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+\n",
      "|salary_id| emp_id|salary_month|  amount|\n",
      "+---------+-------+------------+--------+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|\n",
      "|  4504018|7087500|        NULL| 9370.35|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|\n",
      "+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.amount.cast(\"int\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c43539c5-18d6-4d85-b7a6-fe6ee1d2bbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|  4504017|7087500|  2024-09-25|  NULL|\n",
      "|  4504018|7087500|        NULL|  9370|\n",
      "|  4504019|   NULL|  2024-11-24|  3679|\n",
      "|     NULL|7087500|  2024-12-24|  7730|\n",
      "|  4504021|7087520|  2020-02-19| 14822|\n",
      "+---------+-------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
    "\n",
    "df = df.withColumn(\"amount\", col(\"amount\").cast(IntegerType()))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c8c900e-ef33-4e44-b6f5-1c5d77a8e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'ArrayType', 'Callable', 'Column', 'DataFrame', 'DataType', 'Dict', 'Iterable', 'JVMView', 'List', 'Optional', 'PandasUDFType', 'PySparkTypeError', 'PySparkValueError', 'SparkContext', 'StringType', 'StructType', 'TYPE_CHECKING', 'Tuple', 'Type', 'Union', 'UserDefinedFunction', 'UserDefinedTableFunction', 'ValuesView', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_py_udf', '_create_py_udtf', '_from_numpy_type', '_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_columns', '_invoke_function_over_seq_of_columns', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aes_decrypt', 'aes_encrypt', 'aggregate', 'any_value', 'approxCountDistinct', 'approx_count_distinct', 'approx_percentile', 'array', 'array_agg', 'array_append', 'array_compact', 'array_contains', 'array_distinct', 'array_except', 'array_insert', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_prepend', 'array_remove', 'array_repeat', 'array_size', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bit_and', 'bit_count', 'bit_get', 'bit_length', 'bit_or', 'bit_xor', 'bitmap_bit_position', 'bitmap_bucket_number', 'bitmap_construct_agg', 'bitmap_count', 'bitmap_or_agg', 'bitwiseNOT', 'bitwise_not', 'bool_and', 'bool_or', 'broadcast', 'bround', 'btrim', 'bucket', 'call_function', 'call_udf', 'cardinality', 'cast', 'cbrt', 'ceil', 'ceiling', 'char', 'char_length', 'character_length', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'contains', 'conv', 'convert_timezone', 'corr', 'cos', 'cosh', 'cot', 'count', 'countDistinct', 'count_distinct', 'count_if', 'count_min_sketch', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'csc', 'cume_dist', 'curdate', 'current_catalog', 'current_database', 'current_date', 'current_schema', 'current_timestamp', 'current_timezone', 'current_user', 'date_add', 'date_diff', 'date_format', 'date_from_unix_date', 'date_part', 'date_sub', 'date_trunc', 'dateadd', 'datediff', 'datepart', 'day', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decimal', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'e', 'element_at', 'elt', 'encode', 'endswith', 'equal_null', 'every', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'extract', 'factorial', 'filter', 'find_in_set', 'first', 'first_value', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get', 'get_active_spark_context', 'get_json_object', 'getbit', 'greatest', 'grouping', 'grouping_id', 'has_numpy', 'hash', 'hex', 'histogram_numeric', 'hll_sketch_agg', 'hll_sketch_estimate', 'hll_union', 'hll_union_agg', 'hour', 'hours', 'hypot', 'ifnull', 'ilike', 'initcap', 'inline', 'inline_outer', 'input_file_block_length', 'input_file_block_start', 'input_file_name', 'inspect', 'instr', 'isnan', 'isnotnull', 'isnull', 'java_method', 'json_array_length', 'json_object_keys', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'last_value', 'lcase', 'lead', 'least', 'left', 'length', 'levenshtein', 'like', 'lit', 'ln', 'localtimestamp', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'make_date', 'make_dt_interval', 'make_interval', 'make_timestamp', 'make_timestamp_ltz', 'make_timestamp_ntz', 'make_ym_interval', 'map_concat', 'map_contains_key', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'mask', 'max', 'max_by', 'md5', 'mean', 'median', 'min', 'min_by', 'minute', 'mode', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'named_struct', 'nanvl', 'negate', 'negative', 'next_day', 'now', 'np', 'nth_value', 'ntile', 'nullif', 'nvl', 'nvl2', 'octet_length', 'overlay', 'overload', 'pandas_udf', 'parse_url', 'percent_rank', 'percentile', 'percentile_approx', 'pi', 'pmod', 'posexplode', 'posexplode_outer', 'position', 'positive', 'pow', 'power', 'printf', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'reduce', 'reflect', 'regexp', 'regexp_count', 'regexp_extract', 'regexp_extract_all', 'regexp_instr', 'regexp_like', 'regexp_replace', 'regexp_substr', 'regr_avgx', 'regr_avgy', 'regr_count', 'regr_intercept', 'regr_r2', 'regr_slope', 'regr_sxx', 'regr_sxy', 'regr_syy', 'repeat', 'replace', 'reverse', 'right', 'rint', 'rlike', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'sec', 'second', 'sentences', 'sequence', 'session_window', 'sha', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'sign', 'signum', 'sin', 'sinh', 'size', 'skewness', 'slice', 'some', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'split_part', 'sqrt', 'stack', 'startswith', 'std', 'stddev', 'stddev_pop', 'stddev_samp', 'str_to_map', 'struct', 'substr', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_micros', 'timestamp_millis', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_binary', 'to_char', 'to_csv', 'to_date', 'to_json', 'to_number', 'to_str', 'to_timestamp', 'to_timestamp_ltz', 'to_timestamp_ntz', 'to_unix_timestamp', 'to_utc_timestamp', 'to_varchar', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'try_add', 'try_aes_decrypt', 'try_avg', 'try_divide', 'try_element_at', 'try_multiply', 'try_remote_functions', 'try_subtract', 'try_sum', 'try_to_binary', 'try_to_number', 'try_to_timestamp', 'typeof', 'ucase', 'udf', 'udtf', 'unbase64', 'unhex', 'unix_date', 'unix_micros', 'unix_millis', 'unix_seconds', 'unix_timestamp', 'unwrap_udt', 'upper', 'url_decode', 'url_encode', 'user', 'var_pop', 'var_samp', 'variance', 'version', 'warnings', 'weekday', 'weekofyear', 'when', 'width_bucket', 'window', 'window_time', 'xpath', 'xpath_boolean', 'xpath_double', 'xpath_float', 'xpath_int', 'xpath_long', 'xpath_number', 'xpath_short', 'xpath_string', 'xxhash64', 'year', 'years', 'zip_with']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "print(dir(functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f502658-cad9-4197-a507-d19fcefed604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnsiIntervalType', 'Any', 'ArrayType', 'AtomicType', 'BinaryType', 'BooleanType', 'ByteType', 'Callable', 'CharType', 'ClassVar', 'CloudPickleSerializer', 'DataType', 'DataTypeSingleton', 'DateConverter', 'DateType', 'DatetimeConverter', 'DatetimeNTZConverter', 'DayTimeIntervalType', 'DayTimeIntervalTypeConverter', 'DecimalType', 'Dict', 'DoubleType', 'FloatType', 'FractionalType', 'GatewayClient', 'IntegerType', 'IntegralType', 'Iterable', 'Iterator', 'JVMView', 'JavaClass', 'JavaGateway', 'JavaObject', 'List', 'LongType', 'MapType', 'NullType', 'NumericType', 'NumpyArrayConverter', 'NumpyScalarConverter', 'Optional', 'PySparkNotImplementedError', 'PySparkTypeError', 'PySparkValueError', 'Row', 'ShortType', 'StringType', 'StructField', 'StructType', 'T', 'TYPE_CHECKING', 'TimestampNTZType', 'TimestampType', 'Tuple', 'Type', 'TypeVar', 'U', 'Union', 'UserDefinedType', 'VarcharType', 'YearMonthIntervalType', '_FIXED_DECIMAL', '_INTERVAL_DAYTIME', '_INTERVAL_YEARMONTH', '_LENGTH_CHAR', '_LENGTH_VARCHAR', '__all__', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_acceptable_types', '_all_atomic_types', '_all_complex_types', '_array_signed_int_typecode_ctype_mappings', '_array_type_mappings', '_array_unsigned_int_typecode_ctype_mappings', '_atomic_types', '_complex_types', '_create_converter', '_create_row', '_create_row_inbound_converter', '_from_numpy_type', '_has_nulltype', '_has_type', '_infer_schema', '_infer_type', '_int_size_to_type', '_make_type_verifier', '_merge_type', '_need_converter', '_parse_datatype_json_string', '_parse_datatype_json_value', '_parse_datatype_string', '_test', '_type_mappings', '_typecode', 'array', 'base64', 'calendar', 'cast', 'ctypes', 'datetime', 'decimal', 'dt', 'get_active_spark_context', 'has_numpy', 'json', 'math', 'np', 'overload', 're', 'reduce', 'register_input_converter', 'size', 'sys', 'time']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "print(dir(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2306e56f-691d-474c-ad08-8a14b030b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Alice\", 25.5), (2, \"Bob\", 30.0)]\n",
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10a28307-edcf-46e6-9c25-bf34c47f23c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", FloatType(), True)\n",
    "]\n",
    ")\n",
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True, schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92c5f2dc-dd9a-4263-a1b2-48549965dffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL DDL Schema\n",
    "schema = \"id INT, name STRING, age FLOAT\"\n",
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True, schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc488b17-ac28-41a0-87d4-ed966add2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+\n",
      "| id|     name| age|\n",
      "+---+---------+----+\n",
      "|  1|   Satish|28.2|\n",
      "|  2|   Vamshi|27.3|\n",
      "|  3| Bhagavan|29.1|\n",
      "|  1|    Ameel|28.2|\n",
      "|  2|  Shravan|27.3|\n",
      "|  3|Dhanunjay|29.1|\n",
      "+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL DDL Schema\n",
    "schema = \"id INT, name STRING, age FLOAT\"\n",
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True, schema=schema)\n",
    "df_same_columns = spark.read.csv('data\\\\sample_employee_data_same_columns.txt', header=True, schema=schema)\n",
    "df_u = df.union(df_same_columns)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f475b5db-5674-4627-a9f3-a182a8e19e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  1|    28.2|NULL|\n",
      "|  2|    27.3|NULL|\n",
      "|  3|    29.1|NULL|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SQL DDL Schema\n",
    "schema = \"id INT, name STRING, age FLOAT\"\n",
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True, schema=schema)\n",
    "df_same_columns = spark.read.csv('data\\\\sample_employee_data_order_miss.txt', header=True, schema=schema)\n",
    "df_u = df.union(df_same_columns)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0962d4c6-5ee8-4ab0-9ca7-007a26b99662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_u.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb9eb0a6-2ed8-4c70-9402-90ae062d17a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|    name|     age|\n",
      "+---+--------+--------+\n",
      "|  1|  Satish|    28.2|\n",
      "|  2|  Vamshi|    27.3|\n",
      "|  3|Bhagavan|    29.1|\n",
      "|  1|    28.2|  Satish|\n",
      "|  2|    27.3|  Vamshi|\n",
      "|  3|    29.1|Bhagavan|\n",
      "+---+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True)\n",
    "df_same_columns = spark.read.csv('data\\\\sample_employee_data_order_miss.txt', header=True)\n",
    "df_u = df.union(df_same_columns)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5788df21-7b6c-40b7-b4ad-f360cff1f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True)\n",
    "df_same_columns = spark.read.csv('data\\\\sample_employee_data_order_miss.txt', header=True)\n",
    "df_u = df.unionByName(df_same_columns)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c101cb03-caf8-47e2-950e-aace19ed571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+\n",
      "| id|    name| age|\n",
      "+---+--------+----+\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "|  1|  Satish|28.2|\n",
      "|  2|  Vamshi|27.3|\n",
      "|  3|Bhagavan|29.1|\n",
      "+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True)\n",
    "df_extra_column = spark.read.csv('data\\\\sample_employee_data_extra_col.txt', header=True)\n",
    "df_u = df.unionByName(df_same_columns)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71024d07-081d-4c70-aea9-790b3030aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "| id| age|    name|\n",
      "+---+----+--------+\n",
      "|  1|28.2|  Satish|\n",
      "|  2|27.3|  Vamshi|\n",
      "|  3|29.1|Bhagavan|\n",
      "|  1|28.2|  Satish|\n",
      "|  2|27.3|  Vamshi|\n",
      "|  3|29.1|Bhagavan|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True)\n",
    "df_extra_column = spark.read.csv('data\\\\sample_employee_data_extra_col.txt', header=True)\n",
    "df_u = df_same_columns.unionByName(df)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a71a76e3-b3f8-4ac6-b169-631670d69eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|     age|    name|\n",
      "+---+--------+--------+\n",
      "|  1|    28.2|  Satish|\n",
      "|  2|    27.3|  Vamshi|\n",
      "|  3|    29.1|Bhagavan|\n",
      "|  1|  Satish|    28.2|\n",
      "|  2|  Vamshi|    27.3|\n",
      "|  3|Bhagavan|    29.1|\n",
      "+---+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data\\\\sample_employee_data.txt', header=True)\n",
    "df_extra_column = spark.read.csv('data\\\\sample_employee_data_extra_col.txt', header=True)\n",
    "df_u = df_same_columns.union(df)\n",
    "df_u.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d8335c-1642-4713-b7e9-1942aa82dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea281a80-dba1-4ee2-b9bb-fba83ba05a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data\\\\explode_data.txt',header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0de2c9-8b93-4b35-ab77-5e39be001f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d2ac90-5620-4509-b206-65894cdf0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n",
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|[Laptop, Mouse, K...|2025-03-09|\n",
      "|        102|[Smartphone, Earb...|2025-03-08|\n",
      "|        103|            [Tablet]|2025-03-07|\n",
      "|        104|[Monitor, HDMICab...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "df = spark.read.csv('data\\\\explode_data.txt',header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.withColumn(\"items\",split(col(\"items\"),\";\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79f0ff4-9a1e-48e4-b8b8-8849cd491f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n",
      "+-----------+--------------------+----------+----------+\n",
      "|customer_id|               items|      date|   items_1|\n",
      "+-----------+--------------------+----------+----------+\n",
      "|        101|[Laptop, Mouse, K...|2025-03-09|    Laptop|\n",
      "|        101|[Laptop, Mouse, K...|2025-03-09|     Mouse|\n",
      "|        101|[Laptop, Mouse, K...|2025-03-09|  Keyboard|\n",
      "|        102|[Smartphone, Earb...|2025-03-08|Smartphone|\n",
      "|        102|[Smartphone, Earb...|2025-03-08|   Earbuds|\n",
      "|        103|            [Tablet]|2025-03-07|    Tablet|\n",
      "|        104|[Monitor, HDMICab...|2025-03-06|   Monitor|\n",
      "|        104|[Monitor, HDMICab...|2025-03-06| HDMICable|\n",
      "|        104|[Monitor, HDMICab...|2025-03-06|   Speaker|\n",
      "+-----------+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col,explode\n",
    "df = spark.read.csv('data\\\\explode_data.txt',header=True, inferSchema=True)\n",
    "df.show()\n",
    "df = df.withColumn(\"items\",split(col(\"items\"),\";\"))\n",
    "df.withColumn(\"items_1\", explode('items')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a216c78d-3c4b-45d0-95a4-1632d7342997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n",
      "+-----------+--------------------+----------+--------------------+----------+\n",
      "|customer_id|               items|      date|          items_list|   items_1|\n",
      "+-----------+--------------------+----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop, Mouse, K...|    Laptop|\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop, Mouse, K...|     Mouse|\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop, Mouse, K...|  Keyboard|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|[Smartphone, Earb...|Smartphone|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|[Smartphone, Earb...|   Earbuds|\n",
      "|        103|              Tablet|2025-03-07|            [Tablet]|    Tablet|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor, HDMICab...|   Monitor|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor, HDMICab...| HDMICable|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor, HDMICab...|   Speaker|\n",
      "+-----------+--------------------+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col,explode\n",
    "df = spark.read.csv('data\\\\explode_data.txt',header=True, inferSchema=True)\n",
    "df.show()\n",
    "df = df.withColumn(\"items_list\",split(col(\"items\"),\";\"))\n",
    "df.withColumn(\"items_1\", explode('items_list')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31114dae-fd01-41c3-b146-bae4ee7407ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n",
      "+-----------+--------------------+----------+--------------------+--------------+\n",
      "|customer_id|               items|      date|          items_list|item_has_mouse|\n",
      "+-----------+--------------------+----------+--------------------+--------------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop, Mouse, K...|         false|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|[Smartphone, Earb...|         false|\n",
      "|        103|              Tablet|2025-03-07|            [Tablet]|         false|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor, HDMICab...|         false|\n",
      "+-----------+--------------------+----------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col,explode, array_contains\n",
    "df = spark.read.csv('data\\\\explode_data.txt',header=True, inferSchema=True)\n",
    "df.show()\n",
    "df = df.withColumn(\"items_list\",split(col(\"items\"),\";\"))\n",
    "df.withColumn(\"item_has_mouse\",array_contains(\"items_list\", \"mouse\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a72f66a-c476-4b37-b36c-990f141613f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|               items|      date|\n",
      "+-----------+--------------------+----------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|\n",
      "|        103|              Tablet|2025-03-07|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|\n",
      "+-----------+--------------------+----------+\n",
      "\n",
      "+-----------+--------------------+----------+--------------------+--------------+\n",
      "|customer_id|               items|      date|          items_list|item_has_mouse|\n",
      "+-----------+--------------------+----------+--------------------+--------------+\n",
      "|        101|Laptop;Mouse;Keyb...|2025-03-09|[Laptop, Mouse, K...|          true|\n",
      "|        102|  Smartphone;Earbuds|2025-03-08|[Smartphone, Earb...|         false|\n",
      "|        103|              Tablet|2025-03-07|            [Tablet]|         false|\n",
      "|        104|Monitor;HDMICable...|2025-03-06|[Monitor, HDMICab...|         false|\n",
      "+-----------+--------------------+----------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col,explode, array_contains\n",
    "df = spark.read.csv('data\\\\explode_data.txt',header=True, inferSchema=True)\n",
    "df.show()\n",
    "df = df.withColumn(\"items_list\",split(col(\"items\"),\";\"))\n",
    "df.withColumn(\"item_has_mouse\",array_contains(\"items_list\", \"Mouse\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5619891c-759c-4739-bb25-119402ebf72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, lpad, rpad, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c213c5-cc0d-4c99-a3ae-afa55ab76aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, lpad, rpad, substring\n",
    "df = spark.read.csv(\"data\\\\employee.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc966a8-7560-4e23-a009-ef6d114a1845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|\n",
      "+------+----------+---------+-------+----------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|\n",
      "+------+----------+---------+-------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0260f5e7-cb9f-4e52-8c91-d7b8a58f05de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|first_name_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|            ERIC|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|           HENRY|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|           JAMES|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|          THOMAS|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|            MARY|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|        KIMBERLY|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|           SHAWN|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|          ARTHUR|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|           SUSAN|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|        VICTORIA|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"first_name_upper\",upper(col('first_name'))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68543f35-975e-43b3-860c-4c50477314cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|first_name_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|            eric|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|           henry|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|           james|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|          thomas|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|            mary|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|        kimberly|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|           shawn|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|          arthur|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|           susan|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|        victoria|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, lpad, rpad, substring\n",
    "df.withColumn(\"first_name_upper\",lower(col('first_name'))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157579bc-08bd-4690-a344-03b79af61fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|first_name_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|               4|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|               5|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|               5|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|               6|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|               4|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|               8|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|               5|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|               6|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|               5|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|               8|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, lpad, rpad, substring\n",
    "df.withColumn(\"first_name_upper\",length(col('first_name'))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9155e14c-1f2d-4984-b652-46c7c9edb7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|    first_name_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|gloria50@example.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@exam...|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net| emily15@example.net|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com| ggraves@example.com|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|   xhill@example.com|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|katelyn88@example...|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|sharonking@exampl...|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|chavezrobin@examp...|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  ndavis@example.com|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com| peter54@example.com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|    first_name_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|gloria50@example.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@exam...|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net| emily15@example.net|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com| ggraves@example.com|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|   xhill@example.com|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|katelyn88@example...|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|sharonking@exampl...|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|chavezrobin@examp...|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  ndavis@example.com|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com| peter54@example.com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|    first_name_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|gloria50@example.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@exam...|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net| emily15@example.net|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com| ggraves@example.com|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|   xhill@example.com|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|katelyn88@example...|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|sharonking@exampl...|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|chavezrobin@examp...|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  ndavis@example.com|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com| peter54@example.com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"first_name_upper\",trim(col('email'))).show(10)\n",
    "df.withColumn(\"first_name_upper\",ltrim(col('email'))).show(10)\n",
    "df.withColumn(\"first_name_upper\",rtrim(col('email'))).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ec7f32-e3a4-427c-8f29-f4cbce72c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|            NAME|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|    Eric=>Becker|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|  Henry=>Pacheco|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|   James=>Lawson|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  Thomas=>Harris|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|    Mary=>Harris|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|Kimberly=>Howard|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...| Shawn=>Martinez|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|Arthur=>Richards|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  Susan=>Estrada|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|Victoria=>Taylor|\n",
      "+------+----------+---------+-------+----------+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",concat_ws(\"=>\",col('first_name'), col('last_name'))).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b680475-8cc2-4f9d-846b-7fce544e571f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,concat_ws(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m'\u001b[39m), col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_name\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py:8769\u001b[0m, in \u001b[0;36mconcat_ws\u001b[1;34m(sep, *cols)\u001b[0m\n\u001b[0;32m   8741\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8742\u001b[0m \u001b[38;5;124;03mConcatenates multiple input string columns together into a single string column,\u001b[39;00m\n\u001b[0;32m   8743\u001b[0m \u001b[38;5;124;03musing the given separator.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8766\u001b[0m \u001b[38;5;124;03m[Row(s='abcd-123')]\u001b[39;00m\n\u001b[0;32m   8767\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   8768\u001b[0m sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[1;32m-> 8769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat_ws\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep, _to_seq(sc, cols, _to_java_column))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py:97\u001b[0m, in \u001b[0;36m_invoke_function\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     96\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jf(\u001b[38;5;241m*\u001b[39margs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1314\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m-> 1314\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_args(args)\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[1;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client)\n\u001b[0;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[0;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_collections.py:510\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[1;34m(self, object, gateway_client)\u001b[0m\n\u001b[0;32m    508\u001b[0m ArrayList \u001b[38;5;241m=\u001b[39m JavaClass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.util.ArrayList\u001b[39m\u001b[38;5;124m\"\u001b[39m, gateway_client)\n\u001b[0;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    511\u001b[0m     java_list\u001b[38;5;241m.\u001b[39madd(element)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    720\u001b[0m     )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",concat_ws(col('first_name'), col('last_name'))).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "713484ed-62a1-4021-ab58-28147e59a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+---------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|           NAME|\n",
      "+------+----------+---------+-------+----------+--------------------+---------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|    Eric Becker|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|  Henry Pacheco|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|   James Lawson|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  Thomas Harris|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|    Mary Harris|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|Kimberly Howard|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...| Shawn Martinez|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|Arthur Richards|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  Susan Estrada|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|Victoria Taylor|\n",
      "+------+----------+---------+-------+----------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",concat_ws(\" \",col('first_name'), col('last_name'))).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7547e1c-8495-42f6-88a4-c4993383129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|          NAME|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|    EricBecker|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|  HenryPacheco|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|   JamesLawson|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  ThomasHarris|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|    MaryHarris|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|KimberlyHoward|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...| ShawnMartinez|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|ArthurRichards|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  SusanEstrada|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|VictoriaTaylor|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",concat_ws(\"\",col('first_name'), col('last_name'))).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c96113f-bad9-4a4b-a945-8c8d6e17c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+---------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|     NAME|\n",
      "+------+----------+---------+-------+----------+--------------------+---------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|  1Becker|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...| 2Pacheco|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|  3Lawson|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  4Harris|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|  5Harris|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|  6Howard|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|7Martinez|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|8Richards|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com| 9Estrada|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com| 10Taylor|\n",
      "+------+----------+---------+-------+----------+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",concat_ws(\"\",col('emp_id'), col('last_name'))).show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66e6a058-9256-4d9a-8b25-7afbeb553ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|                NAME|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|gloria50@EXAMple.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@EXAM...|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net| emily15@EXAMple.net|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com| ggraves@EXAMple.com|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|   xhill@EXAMple.com|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|katelyn88@EXAMple...|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|sharonking@EXAMpl...|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|chavezrobin@EXAMp...|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  ndavis@EXAMple.com|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com| peter54@EXAMple.com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",regexp_replace(col('email'), \"exam\",\"EXAM\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd4b261-31ee-4754-9fff-c06516e03913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+-----------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|             NAME|\n",
      "+------+----------+---------+-------+----------+--------------------+-----------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|    gloria50@EXAM|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@EXAM|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|     emily15@EXAM|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|     ggraves@EXAM|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|       xhill@EXAM|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|   katelyn88@EXAM|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|  sharonking@EXAM|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...| chavezrobin@EXAM|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|      ndavis@EXAM|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|     peter54@EXAM|\n",
      "+------+----------+---------+-------+----------+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"NAME\",regexp_replace(col('email'), \"exam.*\",\"EXAM\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a878dad-2358-4356-a008-c874cb5b6de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|is_com|\n",
      "+------+----------+---------+-------+----------+--------------------+------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|  true|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|  true|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|  true|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  true|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|  true|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|  true|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|  true|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|  true|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|  true|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|  true|\n",
      "+------+----------+---------+-------+----------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"is_com\",(col('email').contains('example'))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30fb1587-228b-4ecd-a5ac-0402ef73aceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|  is_com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|oria50@e|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|mancharl|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|ily15@ex|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|raves@ex|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|ill@exam|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|telyn88@|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|aronking|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|avezrobi|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|avis@exa|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|ter54@ex|\n",
      "+------+----------+---------+-------+----------+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"is_com\", substring(col('email'),3,8)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "543d3b5e-9bd8-4798-bb21-3d102f83c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|is_com|\n",
      "+------+----------+---------+-------+----------+--------------------+------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|    gl|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|    ro|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|    em|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|    gg|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|    xh|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|    ka|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|    sh|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|    ch|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|    nd|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|    pe|\n",
      "+------+----------+---------+-------+----------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"is_com\", lpad(col('email'),2,\"*\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf33d197-c25d-48ef-abd0-a6a8dcef2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|              is_com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|gloria50@example.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@example|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|*emily15@example.net|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|*ggraves@example.com|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|***xhill@example.com|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|katelyn88@example.co|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|sharonking@example.o|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|chavezrobin@example.|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|**ndavis@example.com|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|*peter54@example.com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"is_com\", lpad(col('email'),20,\"*\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6936be48-352b-43bb-9456-736b78b6f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email|              is_com|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|gloria50@example.com|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|romancharles@example|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|emily15@example.net*|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|ggraves@example.com*|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|xhill@example.com***|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|katelyn88@example.co|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|sharonking@example.o|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|chavezrobin@example.|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|ndavis@example.com**|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|peter54@example.com*|\n",
      "+------+----------+---------+-------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, substring\n",
    "df.withColumn(\"is_com\", rpad(col('email'),20,\"*\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29043ce2-b01c-4675-b40d-25882a715cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email| new_col|\n",
      "+------+----------+---------+-------+----------+--------------------+--------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|        |\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|        |\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|        |\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  Harris|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|  Harris|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|  Howard|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|Martinez|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|        |\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|        |\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|        |\n",
      "+------+----------+---------+-------+----------+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, \n",
    "    substring, regexp_extract)\n",
    "df.withColumn(\"new_col\", regexp_extract(col('last_name'),'^[HM].+[sdz]$',0)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daf4109f-13da-4e8d-bdda-6350577a7351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------+-------------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email| matched|matched_upper|\n",
      "+------+----------+---------+-------+----------+--------------------+--------+-------------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|        |             |\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|        |             |\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|        |             |\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  Harris|       HARRIS|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|  Harris|       HARRIS|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|  Howard|       HOWARD|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|Martinez|     MARTINEZ|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|        |             |\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|        |             |\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|        |             |\n",
      "+------+----------+---------+-------+----------+--------------------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, \n",
    "    substring, regexp_extract)\n",
    "df = spark.read.csv(\"data\\\\employee.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"matched\", regexp_extract(col('last_name'),'^[HM].+[sdz]$',0))\n",
    "df = df.withColumn(\"matched_upper\", upper(col('matched')))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bacf4b80-8d9c-4f53-af27-62871b3c3366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-------+----------+--------------------+--------+-------------+----------+\n",
      "|emp_id|first_name|last_name|dept_id| hire_date|               email| matched|matched_upper|final_text|\n",
      "+------+----------+---------+-------+----------+--------------------+--------+-------------+----------+\n",
      "|     1|      Eric|   Becker|      9|2016-07-23|gloria50@example.com|        |             |    Becker|\n",
      "|     2|     Henry|  Pacheco|      3|2015-02-23|romancharles@exam...|        |             |   Pacheco|\n",
      "|     3|     James|   Lawson|     49|2018-02-07| emily15@example.net|        |             |    Lawson|\n",
      "|     4|    Thomas|   Harris|     41|2016-09-28| ggraves@example.com|  Harris|       HARRIS|    HARRIS|\n",
      "|     5|      Mary|   Harris|     40|2018-12-18|   xhill@example.com|  Harris|       HARRIS|    HARRIS|\n",
      "|     6|  Kimberly|   Howard|     24|2024-08-24|katelyn88@example...|  Howard|       HOWARD|    HOWARD|\n",
      "|     7|     Shawn| Martinez|     47|2021-06-01|sharonking@exampl...|Martinez|     MARTINEZ|  MARTINEZ|\n",
      "|     8|    Arthur| Richards|      7|2023-12-14|chavezrobin@examp...|        |             |  Richards|\n",
      "|     9|     Susan|  Estrada|     22|2021-12-31|  ndavis@example.com|        |             |   Estrada|\n",
      "|    10|  Victoria|   Taylor|     43|2024-12-07| peter54@example.com|        |             |    Taylor|\n",
      "+------+----------+---------+-------+----------+--------------------+--------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (col, upper, lower, length, concat_ws, regexp_replace, trim, ltrim, rtrim, lpad, rpad, \n",
    "    substring, regexp_extract)\n",
    "df = spark.read.csv(\"data\\\\employee.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"matched\", regexp_extract(col('last_name'),'^[HM].+[sdz]$',0))\n",
    "df = df.withColumn(\"matched_upper\", upper(col('matched')))\n",
    "df = df.withColumn('final_text', regexp_replace(col('last_name'),'^[HM].+[sdz]$',col('matched_upper') ))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfae1b1c-273e-46d4-be04-3b1a7c882875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "print(dir(df.last_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac462c1c-b1d8-4757-8d26-8ea500d022b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(15, 28), match='Harris Howard'>\n",
      "Harris Howard\n",
      "<re.Match object; span=(38, 46), match='Martinez'>\n",
      "Martinez\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Becker Pacheco HARRIS HOWARD Richards MARTINEZ '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = \"Becker Pacheco Harris Howard Richards Martinez \"\n",
    "import re\n",
    "def upper_convert(matched_text):\n",
    "    print(matched_text)\n",
    "    print(matched_text.group())\n",
    "    return matched_text.group().upper()\n",
    "re.sub(\"[HM].+?[dz]\", upper_convert, string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595ed193-0020-4d05-b3c3-d902e8551cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, lpad, rpad, substring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec0617-ec20-46c8-b6be-354755e3ee86",
   "metadata": {},
   "source": [
    "# JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e01d4a53-4168-4bdc-8a44-2f8eb2986b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+\n",
      "|user_id|   name|              email|\n",
      "+-------+-------+-------------------+\n",
      "|      1|  Alice|  alice@example.com|\n",
      "|      2|    Bob|    bob@example.com|\n",
      "|      3|Charlie|charlie@example.com|\n",
      "|      4|  David|  david@example.com|\n",
      "+-------+-------+-------------------+\n",
      "\n",
      "+--------+-------+-----+-------------------+\n",
      "|click_id|user_id|ad_id|         click_time|\n",
      "+--------+-------+-----+-------------------+\n",
      "|     101|      1| 5001|2025-03-10 10:00:00|\n",
      "|     102|      2| 5002|2025-03-10 10:05:00|\n",
      "|     103|      1| 5003|2025-03-10 10:10:00|\n",
      "|     104|      3| 5004|2025-03-10 10:20:00|\n",
      "|     107|      5| 5004|2025-03-10 10:20:00|\n",
      "+--------+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df = spark.read.csv(\"data\\\\joins\\\\users.csv\", header=True, inferSchema=True)\n",
    "click_logs_df = spark.read.csv(\"data\\\\joins\\\\click_logs.csv\", header=True, inferSchema=True)\n",
    "users_df.show()\n",
    "click_logs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1476e3-0582-4ee5-ba29-3573b404aa93",
   "metadata": {},
   "source": [
    "## check who cliked on our add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb44693-bad0-42d7-b033-196d173a6f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "|user_id|   name|              email|click_id|user_id|ad_id|         click_time|\n",
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "|      1|  Alice|  alice@example.com|     101|      1| 5001|2025-03-10 10:00:00|\n",
      "|      2|    Bob|    bob@example.com|     102|      2| 5002|2025-03-10 10:05:00|\n",
      "|      1|  Alice|  alice@example.com|     103|      1| 5003|2025-03-10 10:10:00|\n",
      "|      3|Charlie|charlie@example.com|     104|      3| 5004|2025-03-10 10:20:00|\n",
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(click_logs_df, users_df['user_id']==click_logs_df['user_id'], \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db0a9b-5410-4e9c-931e-da483ba7cba0",
   "metadata": {},
   "source": [
    "## You want to get a list of all adds, including those who have not belongs to perticular user (right join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925274cb-349a-43f6-a8c7-3cf51fd10cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "|user_id|   name|              email|click_id|user_id|ad_id|         click_time|\n",
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "|      1|  Alice|  alice@example.com|     101|      1| 5001|2025-03-10 10:00:00|\n",
      "|      2|    Bob|    bob@example.com|     102|      2| 5002|2025-03-10 10:05:00|\n",
      "|      1|  Alice|  alice@example.com|     103|      1| 5003|2025-03-10 10:10:00|\n",
      "|      3|Charlie|charlie@example.com|     104|      3| 5004|2025-03-10 10:20:00|\n",
      "|   NULL|   NULL|               NULL|     107|      5| 5004|2025-03-10 10:20:00|\n",
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(click_logs_df, users_df['user_id']==click_logs_df['user_id'], \"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b037996-fe0c-42e5-8553-cf8339e7066a",
   "metadata": {},
   "source": [
    "## You want to get a list of all users, including those who have not clicked an add (left join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d660f233-9204-4fe8-b9e3-e8d7e02f022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "|user_id|   name|              email|click_id|user_id|ad_id|         click_time|\n",
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "|      1|  Alice|  alice@example.com|     103|      1| 5003|2025-03-10 10:10:00|\n",
      "|      1|  Alice|  alice@example.com|     101|      1| 5001|2025-03-10 10:00:00|\n",
      "|      2|    Bob|    bob@example.com|     102|      2| 5002|2025-03-10 10:05:00|\n",
      "|      3|Charlie|charlie@example.com|     104|      3| 5004|2025-03-10 10:20:00|\n",
      "|      4|  David|  david@example.com|    NULL|   NULL| NULL|               NULL|\n",
      "+-------+-------+-------------------+--------+-------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(click_logs_df, users_df['user_id']==click_logs_df['user_id'], \"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23337d0-4541-408a-99ad-2274f44cb0fe",
   "metadata": {},
   "source": [
    "## Check who not clicked on our add(anti join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce13ae92-199f-42c6-b12d-b2a276e4886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n",
      "|user_id| name|            email|\n",
      "+-------+-----+-----------------+\n",
      "|      4|David|david@example.com|\n",
      "+-------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_df.join(click_logs_df, users_df['user_id']==click_logs_df['user_id'], \"anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d1000-ea7b-4956-8461-d3ca21230e2d",
   "metadata": {},
   "source": [
    "## Fraud Detection in Banking Transactions (SELF JOIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eea1631d-92b2-43d7-987b-147adb03bdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+-------------+----------------+\n",
      "|transaction_id|user_id|amount|     location|transaction_time|\n",
      "+--------------+-------+------+-------------+----------------+\n",
      "|           201|      1|   500|     New York|10-03-2025 12:00|\n",
      "|           202|      1|   700|  Los Angeles|10-03-2025 12:04|\n",
      "|           203|      2|   200|      Chicago|10-03-2025 14:00|\n",
      "|           204|      2|   250|      Chicago|10-03-2025 14:01|\n",
      "|           205|      3|  1000|San Francisco|10-03-2025 15:30|\n",
      "|           206|      3|  1500|    Las Vegas|10-03-2025 15:34|\n",
      "+--------------+-------+------+-------------+----------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transaction_df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "transaction_df.show()\n",
    "transaction_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08e41634-65c4-494f-ba01-43578a89a837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transaction_id: int, user_id: int, amount: int, location: string, transaction_time: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "transaction_df.show()\n",
    "transaction_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7048a8-1f8a-4343-8e88-f51623500917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
