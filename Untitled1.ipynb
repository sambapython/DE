{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d20939-2d83-4299-9437-e78aa68cad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155a197b-c00f-408d-af12-82ce32e0b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfbfc3-7873-4a6e-aac3-bac7dcb2da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18b3c76-d0fe-4834-a607-693b21abef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a9dcb3-a703-44fc-9622-cf2aa3fb57a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m3.\u001b[39m, \u001b[38;5;241m4.\u001b[39m,\u001b[38;5;241m5.\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring3\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m: [date(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), date(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), date(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m: [datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m), datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m), datetime(\u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m0\u001b[39m)]\n\u001b[0;32m      7\u001b[0m })\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(pandas_df)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3,4],\n",
    "    'b': [2., 3., 4.,5.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551b7fc1-f225-462e-ae9b-5b5af7fe4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|5.0|   NULL|      NULL|               NULL|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': pd.Series([1, 2, 3,4]),\n",
    "    'b': pd.Series([2., 3., 4.,5.]),\n",
    "    'c': pd.Series(['string1', 'string2', 'string3']),\n",
    "    'd': pd.Series([date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)]),\n",
    "    'e': pd.Series([datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)])\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8206010b-9aed-43cf-9439-93e0b96b023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a    b        c           d                   e\n",
      "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
      "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
      "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00\n",
      "3  4  5.0      NaN         NaN                 NaT\n"
     ]
    }
   ],
   "source": [
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4687afb-f8ab-4864-a611-49c61f5c2d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+----+-------+----------+-------------------+\n",
      "|  a|   b|      c|         d|                  e|\n",
      "+---+----+-------+----------+-------------------+\n",
      "|  1| 2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2| 3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3| 4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "|  4|NULL|   NULL|      NULL|               NULL|\n",
      "+---+----+-------+----------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': pd.Series([1, 2, 3,4]),\n",
    "    'b': pd.Series([2., 3., 4.]),\n",
    "    'c': pd.Series(['string1', 'string2', 'string3']),\n",
    "    'd': pd.Series([date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)]),\n",
    "    'e': pd.Series([datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)])\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "print(df)\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62f6b39-d855-4f46-bf20-211ee45160d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'b'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb3a8ac-4a29-4734-b4f2-ca70efd7c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "print(dir(df.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed98a124-bd11-4fb9-a10f-463fbc3513e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[b: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac358a26-1680-4fce-bab2-ba965fb99bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'agg', 'alias', 'approxQuantile', 'b', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n",
      "********************\n",
      "['__add__', '__and__', '__bool__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__invert__', '__iter__', '__le__', '__lt__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_asc_doc', '_asc_nulls_first_doc', '_asc_nulls_last_doc', '_bitwiseAND_doc', '_bitwiseOR_doc', '_bitwiseXOR_doc', '_contains_doc', '_desc_doc', '_desc_nulls_first_doc', '_desc_nulls_last_doc', '_endswith_doc', '_eqNullSafe_doc', '_isNotNull_doc', '_isNull_doc', '_jc', '_startswith_doc', 'alias', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'astype', 'between', 'bitwiseAND', 'bitwiseOR', 'bitwiseXOR', 'cast', 'contains', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'dropFields', 'endswith', 'eqNullSafe', 'getField', 'getItem', 'ilike', 'isNotNull', 'isNull', 'isin', 'like', 'name', 'otherwise', 'over', 'rlike', 'startswith', 'substr', 'when', 'withField']\n"
     ]
    }
   ],
   "source": [
    "df_b = df.select(\"b\")\n",
    "df_col_b = df.b\n",
    "print(dir(df_b))\n",
    "print(\"*\"*20)\n",
    "print(dir(df_col_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16345bbc-08e6-49e3-9f8d-31df71844e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"b\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b802233a-17d4-4493-828c-3bc5f1fb1c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d242880-9917-483c-a796-3f0b66a399ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d293a83-9c89-4353-a152-1f6e2bc830b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93470b0c-043b-4c3e-8daf-578fd43f32b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5974f3a8-74e4-47d6-a5c0-9066ba2ee4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf9dae0-214c-4eb4-a24b-056f378973e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "     a\n",
      "0  1.0\n",
      "1  2.0\n",
      "2  NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "343fe5dc-8d2a-459e-abeb-b0ce13fd91cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b84b62c-be1e-4ec0-9041-648196eb027f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    2.0\n",
       "2    NaN\n",
       "Name: a, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f5e7435-ab12-436e-a65e-fb08ed5bd6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df1.a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3982e36-eac4-4f87-8d34-e29b124b678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0df53d17-1e98-4288-88f5-2a4e2cc67a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "+----+\n",
      "|   a|\n",
      "+----+\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "|NULL|\n",
      "+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c5f9ca1-e1ae-4b91-9ddf-efb0a8313a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "-RECORD 0---\n",
      " a   | 1.0  \n",
      "-RECORD 1---\n",
      " a   | 2.0  \n",
      "-RECORD 2---\n",
      " a   | NULL \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "print(sdf.show(vertical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51d546eb-6531-416c-8cb3-bcbf67a4a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "-RECORD 0---\n",
      " a   | 1.0  \n",
      "-RECORD 1---\n",
      " a   | 2.0  \n",
      "-RECORD 2---\n",
      " a   | NULL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "sdf.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "958f9d19-371f-40aa-8593-b951b1daa455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double]\n",
      "-RECORD 0--\n",
      " a   | 1.0 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "sdf.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57e8e230-d274-46dd-83e4-9f783310bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "DataFrame[a: double, b: double]\n",
      "-RECORD 0--\n",
      " a   | 1.0 \n",
      " b   | 1.0 \n",
      "-RECORD 1--\n",
      " a   | 2.0 \n",
      " b   | 2.0 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)\n",
    "sdf.show(2,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac794e14-a74c-4be4-bed9-af04eafe5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "| 1.0| 1.0|\n",
      "| 2.0| 2.0|\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fd5b93f-86dd-4a55-ad88-7db8712f903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "| 1.0| 1.0|\n",
      "| 2.0| 2.0|\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
       "\n",
       ".. versionadded:: 3.0.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "func : function\n",
       "    a function that takes and returns a :class:`DataFrame`.\n",
       "*args\n",
       "    Positional arguments to pass to func.\n",
       "\n",
       "    .. versionadded:: 3.3.0\n",
       "**kwargs\n",
       "    Keyword arguments to pass to func.\n",
       "\n",
       "    .. versionadded:: 3.3.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    Transformed DataFrame.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql.functions import col\n",
       ">>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
       ">>> def cast_all_to_int(input_df):\n",
       "...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
       "...\n",
       ">>> def sort_columns_asc(input_df):\n",
       "...     return input_df.select(*sorted(input_df.columns))\n",
       "...\n",
       ">>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
       "+-----+---+\n",
       "|float|int|\n",
       "+-----+---+\n",
       "|    1|  1|\n",
       "|    2|  2|\n",
       "+-----+---+\n",
       "\n",
       ">>> def add_n(input_df, n):\n",
       "...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
       "...                             for col_name in input_df.columns])\n",
       ">>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
       "+---+-----+\n",
       "|int|float|\n",
       "+---+-----+\n",
       "| 12| 12.0|\n",
       "| 13| 13.0|\n",
       "+---+-----+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.show())\n",
    "sdf.transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a9ebd-097d-4441-8086-4b549500d2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4851bd5-8ec1-454d-950c-f9a02e1857dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function col at 0x0000025A6BA04400>\n",
      "Column<'col1'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "print(col)\n",
    "print(col(\"col1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "216093b4-8843-453a-83af-3fd66bf747d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "3.0\n",
      "float64\n",
      "3.0\n",
      "+----+----+\n",
      "|   a|   b|\n",
      "+----+----+\n",
      "| 1.0| 1.0|\n",
      "| 2.0| 2.0|\n",
      "|NULL|NULL|\n",
      "+----+----+\n",
      "\n",
      "None\n",
      "converted\n",
      "DataFrame[a: double, b: double]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[a: double, b: double]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(1+2+np.nan)\n",
    "print(sum([1,2,np.nan]))\n",
    "ser1 = pd.Series([1,2,np.nan])\n",
    "print(ser1.sum())\n",
    "print(ser1.dtype)\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "print(df1.a.sum())\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.show())\n",
    "def convert(data):\n",
    "    print(\"converted\")\n",
    "    print(data)\n",
    "    return data\n",
    "sdf.transform(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e8bb327-e55a-4654-aaac-deed3995e1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted\n",
      "DataFrame[a: double, b: double]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[a: double, b: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,np.nan]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "def convert(data):\n",
    "    print(\"converted\")\n",
    "    print(data)\n",
    "    return data\n",
    "sdf.transform(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c5e2d6-e911-44b7-952c-36d108a5e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ec4fa9-73a6-42ff-abde-867a7f073d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  2| 2.0|\n",
      "|  3|NULL|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1b41ec-1419-4025-b9b5-3493bfdc8c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fd3f67-8561-44c8-950a-08ce4d3f3f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'CAST(a AS FLOAT)'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.a.cast(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cce006-fad2-400c-80f9-109a2c285a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e50329a-1dee-46a2-ac08-2d77f1bd72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a.cast(\"double\")\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac2fcf0-5163-4462-9877-049b578df2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"double\")\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9729d7db-d042-4cdb-84b9-62f7554a62a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6fd2751-5aff-4331-b8de-baf57e793fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'a', 'agg', 'alias', 'approxQuantile', 'b', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
     ]
    }
   ],
   "source": [
    "print(dir(sdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674690e7-4c9f-46ee-89ee-f9bb278f37b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n",
      "DataFrame[a: bigint, b: double]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)\n",
    "sdf.drop(\"a\")\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b270741f-5ae7-47dd-998a-d5bcd60456ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n",
      "DataFrame[a: bigint, b: double]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'ColumnOrName'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Returns a new :class:`DataFrame` without specified columns.\n",
       "This is a no-op if the schema doesn't contain the given column name(s).\n",
       "\n",
       ".. versionadded:: 1.4.0\n",
       "\n",
       ".. versionchanged:: 3.4.0\n",
       "    Supports Spark Connect.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "cols: str or :class:`Column`\n",
       "    a name of the column, or the :class:`Column` to drop\n",
       "\n",
       "Returns\n",
       "-------\n",
       ":class:`DataFrame`\n",
       "    DataFrame without given columns.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When an input is a column name, it is treated literally without further interpretation.\n",
       "Otherwise, will try to match the equivalent expression.\n",
       "So that dropping column by its name `drop(colName)` has different semantic with directly\n",
       "dropping the column `drop(col(colName))`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from pyspark.sql import Row\n",
       ">>> from pyspark.sql.functions import col, lit\n",
       ">>> df = spark.createDataFrame(\n",
       "...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
       ">>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
       "\n",
       ">>> df.drop('age').show()\n",
       "+-----+\n",
       "| name|\n",
       "+-----+\n",
       "|  Tom|\n",
       "|Alice|\n",
       "|  Bob|\n",
       "+-----+\n",
       ">>> df.drop(df.age).show()\n",
       "+-----+\n",
       "| name|\n",
       "+-----+\n",
       "|  Tom|\n",
       "|Alice|\n",
       "|  Bob|\n",
       "+-----+\n",
       "\n",
       "Drop the column that joined both DataFrames on.\n",
       "\n",
       ">>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
       "+---+------+\n",
       "|age|height|\n",
       "+---+------+\n",
       "| 14|    80|\n",
       "| 16|    85|\n",
       "+---+------+\n",
       "\n",
       ">>> df3 = df.join(df2)\n",
       ">>> df3.show()\n",
       "+---+-----+------+----+\n",
       "|age| name|height|name|\n",
       "+---+-----+------+----+\n",
       "| 14|  Tom|    80| Tom|\n",
       "| 14|  Tom|    85| Bob|\n",
       "| 23|Alice|    80| Tom|\n",
       "| 23|Alice|    85| Bob|\n",
       "| 16|  Bob|    80| Tom|\n",
       "| 16|  Bob|    85| Bob|\n",
       "+---+-----+------+----+\n",
       "\n",
       "Drop two column by the same name.\n",
       "\n",
       ">>> df3.drop(\"name\").show()\n",
       "+---+------+\n",
       "|age|height|\n",
       "+---+------+\n",
       "| 14|    80|\n",
       "| 14|    85|\n",
       "| 23|    80|\n",
       "| 23|    85|\n",
       "| 16|    80|\n",
       "| 16|    85|\n",
       "+---+------+\n",
       "\n",
       "Can not drop col('name') due to ambiguous reference.\n",
       "\n",
       ">>> df3.drop(col(\"name\")).show()\n",
       "Traceback (most recent call last):\n",
       "...\n",
       "pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference...\n",
       "\n",
       ">>> df4 = df.withColumn(\"a.b.c\", lit(1))\n",
       ">>> df4.show()\n",
       "+---+-----+-----+\n",
       "|age| name|a.b.c|\n",
       "+---+-----+-----+\n",
       "| 14|  Tom|    1|\n",
       "| 23|Alice|    1|\n",
       "| 16|  Bob|    1|\n",
       "+---+-----+-----+\n",
       "\n",
       ">>> df4.drop(\"a.b.c\").show()\n",
       "+---+-----+\n",
       "|age| name|\n",
       "+---+-----+\n",
       "| 14|  Tom|\n",
       "| 23|Alice|\n",
       "| 16|  Bob|\n",
       "+---+-----+\n",
       "\n",
       "Can not find a column matching the expression \"a.b.c\".\n",
       "\n",
       ">>> df4.drop(col(\"a.b.c\")).show()\n",
       "+---+-----+-----+\n",
       "|age| name|a.b.c|\n",
       "+---+-----+-----+\n",
       "| 14|  Tom|    1|\n",
       "| 23|Alice|    1|\n",
       "| 16|  Bob|    1|\n",
       "+---+-----+-----+\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\navya\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)\n",
    "sdf.drop?\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5228faa8-5027-4a33-8250-f5a07e4fadff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "[('a', 'bigint'), ('b', 'double')]\n",
      "+----+\n",
      "|   b|\n",
      "+----+\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "|NULL|\n",
      "+----+\n",
      "\n",
      "DataFrame[a: bigint, b: double]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "print(sdf.dtypes)\n",
    "sdf.drop(\"a\").show()\n",
    "print(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5557bcd-8f92-4ee5-9f96-8744ef52e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  2| 2.0|\n",
      "|  3|NULL|\n",
      "+---+----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "sdf.c=pd.Series([1,2,3])\n",
    "print(sdf.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4e181e-ef15-4116-bb0d-b6adfd26283d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'bigint'), ('b', 'double')]\n",
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  2| 2.0|\n",
      "|  3|NULL|\n",
      "+---+----+\n",
      "\n",
      "None\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'a', 'agg', 'alias', 'approxQuantile', 'b', 'c', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\"a\":pd.Series([1,2,3]), \"b\":pd.Series([1,2,np.nan])})\n",
    "sdf = spark.createDataFrame(df1)\n",
    "print(sdf.dtypes)\n",
    "sdf.a = sdf.a.cast(\"string\")\n",
    "sdf.c=pd.Series([1,2,3])\n",
    "print(sdf.show())\n",
    "print(dir(sdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937909b3-a820-48f1-9d8e-bf385f5c3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eb04ca-4bd8-4b3f-97fd-e6ff61ae5a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477845e3-8096-4582-a7ad-1ba74fd8924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JobTracking\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d0b0c3-59a0-4ee8-8de6-7efc75ec57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ebaa95b-ba07-45f8-8ce7-07ed496dc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JobTracking\") \\\n",
    "    .config(\"spark.ui.port\", \"4041\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68b2e86-81ea-49ec-a3aa-9601ec27cbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Navya\\AppData\\Local\\Temp\\ipykernel_6684\\2161328399.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df_1cr = spark.read.csv(\"data\\salary_1cr.csv\", inferSchema=True, header=True)\n"
     ]
    }
   ],
   "source": [
    "df_1cr = spark.read.csv(\"data\\salary_1cr.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba7ab58-08f4-4efd-a7db-b21172e40c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[salary_id: int, emp_id: int, salary_month: date, amount: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e02b5f-28ec-41da-b77e-492e73f67aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1cr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0cbe7d-cac5-4e87-b68a-a3f9ef5d49f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary_id', 'emp_id', 'salary_month', 'amount']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1cr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "489f9b9c-77c4-4354-895c-9b2d15fc4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|  emp_id|salary_month|\n",
      "+--------+------------+\n",
      "|39470031|  2020-10-16|\n",
      "|39470031|  2020-11-15|\n",
      "+--------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a one more df with some columns\n",
    "df_emp_id_amount = df_1cr.select(\"emp_id\",\"salary_month\")\n",
    "df_emp_id_amount.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf3d95-3f8d-4de9-b4f0-9926664d4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a one more df with some columns\n",
    "df_emp_id_amount = df_1cr.select(\"emp_id\",\"salary_month\")\n",
    "print(dir(df_emp_id_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546866c6-4fa7-4caa-88fd-9bb0a2cfbd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|  emp_id|salary_month|\n",
      "+--------+------------+\n",
      "|39470031|  2020-10-16|\n",
      "|39470031|  2020-11-15|\n",
      "|39470031|  2020-12-15|\n",
      "+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the standardise way pyspark sugesting is using col function\n",
    "from pyspark.sql.functions import col\n",
    "df_emp_id_amount = df_1cr.select(col(\"emp_id\"),col(\"salary_month\"))\n",
    "print(df_emp_id_amount.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a945673-93fe-4675-94bf-4f35a5124f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|  emp_id|     month|\n",
      "+--------+----------+\n",
      "|39470031|2020-10-16|\n",
      "|39470031|2020-11-15|\n",
      "|39470031|2020-12-15|\n",
      "+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ALIAS: change the column name if you dont like\n",
    "df_emp_id_amount = df_1cr.select(col(\"emp_id\"),col(\"salary_month\").alias('month'))\n",
    "print(df_emp_id_amount.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac32c4ce-70c3-4855-8fed-b6993614dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "#Filter\n",
    "df_39470031 = df_1cr.filter(col(\"emp_id\")==39470031)\n",
    "print(df_39470031.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af55b76f-f6e6-4443-8e35-6b074669a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|  4504017|7087500|  2024-09-25|  NULL|\n",
      "+---------+-------+------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Filter 7087500 employee january month results\n",
    "from pyspark.sql.functions import col, month\n",
    "df_sal50 = spark.read.csv('data\\\\salary50.txt',header=True, inferSchema=True)\n",
    "df_7087500 = df_sal50.filter((col(\"emp_id\")==7087500) & (month(col(\"salary_month\"))==9))\n",
    "print(df_7087500.count())\n",
    "print(df_7087500.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2969e3-108a-436c-996d-57a382d156f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|  4504017|7087500|  2024-09-25|  NULL|\n",
      "|     NULL|7087500|  2024-12-24|7730.6|\n",
      "+---------+-------+------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Filter 7087500 employee january month results\n",
    "#isin?...  9,12\n",
    "from pyspark.sql.functions import col, month\n",
    "df_sal50 = spark.read.csv('data\\\\salary50.txt',header=True, inferSchema=True)\n",
    "df_7087500 = df_sal50.filter((col(\"emp_id\")==7087500) & (month(col(\"salary_month\")).isin(9,12)))\n",
    "print(df_7087500.count())\n",
    "print(df_7087500.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d35b4cd3-9cee-4ffb-bdcb-47febfbaffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+---------+-------+------------+------+\n",
      "|salary_id| emp_id|salary_month|amount|\n",
      "+---------+-------+------------+------+\n",
      "|     NULL|7087500|  2024-12-24|7730.6|\n",
      "+---------+-------+------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Filter 7087500 employee january month results\n",
    "#isin?...  9,12\n",
    "from pyspark.sql.functions import col, month\n",
    "df_sal50 = spark.read.csv('data\\\\salary50.txt',header=True, inferSchema=True)\n",
    "df_7087500 = df_sal50.filter((col(\"emp_id\")==7087500) & (month(col(\"salary_month\")).isin(9,12)) & col('salary_id').isNull())\n",
    "print(df_7087500.count())\n",
    "print(df_7087500.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc123bf-5add-4730-b080-be4e27201abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1cr = spark.read.csv(\"data\\\\salary_1cr.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fd6bc56-1db0-4855-b0c6-97ba8db5cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[salary_id: int, emp_id: int, salary_month: date, amount: double]\n"
     ]
    }
   ],
   "source": [
    "# rename the columns\n",
    "print(df_1cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52bd2df8-1766-47c6-b226-0fdc2d5602d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[salary_id: int, emp_id: int, salary_month: date, amount: double]\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('salary_id',\"sal_id\")\n",
    "print(df_1cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cbba348-5063-4e0d-93bc-ec8daa8ec2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salary_id', 'emp_id', 'salary_month', 'amount']\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('salary_id',\"sal_id\")\n",
    "print(df_1cr.columns) # dolumns are not actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fc744a6-9888-4444-af46-64ad2c6926de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-------+\n",
      "|salary_id|  emp_id|salary_month| amount|\n",
      "+---------+--------+------------+-------+\n",
      "|  9505409|39470031|  2020-10-16|7710.72|\n",
      "|  9505410|39470031|  2020-11-15|3114.26|\n",
      "|  9505411|39470031|  2020-12-15|10102.4|\n",
      "+---------+--------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('salary_id',\"sal_id\")\n",
    "df_1cr.show(3) # show is an actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43b9b6c2-4139-4052-8e86-fb55c5a1cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-------+\n",
      "|salary_id|  emp_id|salary_month| amount|\n",
      "+---------+--------+------------+-------+\n",
      "|  9505409|39470031|  2020-10-16|7710.72|\n",
      "|  9505410|39470031|  2020-11-15|3114.26|\n",
      "|  9505411|39470031|  2020-12-15|10102.4|\n",
      "+---------+--------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1cr.withColumnRenamed('emp_id',\"empid\")\n",
    "df_1cr.show(3) # show is an actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafdf69-0f2f-46ed-8684-3e8a78a71cc3",
   "metadata": {},
   "source": [
    "## the transformations will not change the existing dataframe. The SPRAK DATAFRAME IS IMMUTABLE OBJECT TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "213fa3df-ba96-4600-8cc8-a83093766a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+-------+\n",
      "|salary_id|   empid|salary_month| amount|\n",
      "+---------+--------+------------+-------+\n",
      "|  9505409|39470031|  2020-10-16|7710.72|\n",
      "|  9505410|39470031|  2020-11-15|3114.26|\n",
      "|  9505411|39470031|  2020-12-15|10102.4|\n",
      "+---------+--------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1cr_renamed = df_1cr.withColumnRenamed('emp_id',\"empid\")\n",
    "df_1cr_renamed.show(3) # show is an actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79e63c-0e0f-47e9-a9a4-2d21c0a9200e",
   "metadata": {},
   "source": [
    "## ALL TRNASFORMATIONS RETURNS THE NEW DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b5631-c174-45aa-b388-e94b5043a071",
   "metadata": {},
   "source": [
    "### withColumn\n",
    "#### helps us to create new column or change the existing column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0698dc3-b03b-4b97-805b-86f3f57ecd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4198691d-409a-45f7-a66a-e2354b55b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c502d6-dd57-4f54-81bb-f854957d1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data\\\\salary_1cr.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c30d4022-d641-4a57-9f51-a2f14d767371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary_id', 'emp_id', 'salary_month', 'amount']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3423d3-2fa5-4330-a6ba-5e1670977e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7de159a-f21c-4767-bd6e-3203215367be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"YEAR\",year(df.salary_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ef191b-8e08-41bb-8ffe-2c8f048e1114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salary_id', 'emp_id', 'salary_month', 'amount', 'YEAR']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf6e1e5-78c6-43ab-ac93-497b85d20c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------+----+\n",
      "|salary_id|  emp_id|salary_month|  amount|YEAR|\n",
      "+---------+--------+------------+--------+----+\n",
      "|  9505409|39470031|  2020-10-16| 7710.72|2020|\n",
      "|  9505410|39470031|  2020-11-15| 3114.26|2020|\n",
      "|  9505411|39470031|  2020-12-15| 10102.4|2020|\n",
      "|  9505412|39470031|  2021-01-14|13129.35|2020|\n",
      "|  9505413|39470031|  2021-02-13|  5642.5|2020|\n",
      "|  9505414|39470031|  2021-03-15|  7808.9|2020|\n",
      "|  9505415|39470031|  2021-04-14|  5634.8|2020|\n",
      "|  9505416|39470031|  2021-05-14|  9629.5|2020|\n",
      "|  9505417|39470031|  2021-06-13| 14030.1|2020|\n",
      "|  9505418|39470031|  2021-07-13|  6550.5|2020|\n",
      "+---------+--------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81947c8d-c90a-4fea-b968-3157975ac1ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got str.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5172\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5130\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[0;32m   5131\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5169\u001b[0m \u001b[38;5;124;03m+---+-----+----+\u001b[39;00m\n\u001b[0;32m   5170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m-> 5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[0;32m   5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_COLUMN] Argument `col` should be a Column, got str."
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"YEAR\",\"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e036fd0-34ae-4e64-85ee-9a30225e2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------+----+\n",
      "|salary_id|  emp_id|salary_month|  amount|YEAR|\n",
      "+---------+--------+------------+--------+----+\n",
      "|  9505409|39470031|  2020-10-16| 7710.72|2020|\n",
      "|  9505410|39470031|  2020-11-15| 3114.26|2020|\n",
      "|  9505411|39470031|  2020-12-15| 10102.4|2020|\n",
      "|  9505412|39470031|  2021-01-14|13129.35|2020|\n",
      "|  9505413|39470031|  2021-02-13|  5642.5|2020|\n",
      "|  9505414|39470031|  2021-03-15|  7808.9|2020|\n",
      "|  9505415|39470031|  2021-04-14|  5634.8|2020|\n",
      "|  9505416|39470031|  2021-05-14|  9629.5|2020|\n",
      "|  9505417|39470031|  2021-06-13| 14030.1|2020|\n",
      "|  9505418|39470031|  2021-07-13|  6550.5|2020|\n",
      "+---------+--------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = df.withColumn(\"YEAR\",lit(\"2020\"))\n",
    "print(df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "951d2bc2-506b-4ee9-b400-3baef0a2a30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------+----+\n",
      "|salary_id|  emp_id|salary_month|  amount|YEAR|\n",
      "+---------+--------+------------+--------+----+\n",
      "|  9505409|39470031|  2020-10-16| 7710.72|NULL|\n",
      "|  9505410|39470031|  2020-11-15| 3114.26|NULL|\n",
      "|  9505411|39470031|  2020-12-15| 10102.4|NULL|\n",
      "|  9505412|39470031|  2021-01-14|13129.35|NULL|\n",
      "|  9505413|39470031|  2021-02-13|  5642.5|NULL|\n",
      "|  9505414|39470031|  2021-03-15|  7808.9|NULL|\n",
      "|  9505415|39470031|  2021-04-14|  5634.8|NULL|\n",
      "|  9505416|39470031|  2021-05-14|  9629.5|NULL|\n",
      "|  9505417|39470031|  2021-06-13| 14030.1|NULL|\n",
      "|  9505418|39470031|  2021-07-13|  6550.5|NULL|\n",
      "+---------+--------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = df.withColumn(\"YEAR\",lit(None))\n",
    "print(df.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "390c66fe-7b1d-445b-a4a7-b85718890176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+\n",
      "|salary_id| emp_id|salary_month|  amount|\n",
      "+---------+-------+------------+--------+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|\n",
      "|  4504018|7087500|        NULL| 9370.35|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|\n",
      "|  4504022|7087520|  2020-03-20| 6668.04|\n",
      "|  4504023|7087520|  2020-04-19|  5948.5|\n",
      "|  4504024|7087520|  2020-05-19| 6760.61|\n",
      "|  4504025|7087520|  2020-06-18|12243.51|\n",
      "|  4504026|7087520|  2020-07-18| 3069.33|\n",
      "+---------+-------+------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# doe the withColumn handles the NULL values?\n",
    "df_50null = spark.read.csv(\"data\\\\salary50.txt\", header=True, inferSchema=True)\n",
    "df_50null.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62a82d63-4d00-4328-9987-df45d52fe5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+--------+----+\n",
      "|salary_id| emp_id|salary_month|  amount|YEAR|\n",
      "+---------+-------+------------+--------+----+\n",
      "|  4504017|7087500|  2024-09-25|    NULL|2024|\n",
      "|  4504018|7087500|        NULL| 9370.35|NULL|\n",
      "|  4504019|   NULL|  2024-11-24| 3679.19|2024|\n",
      "|     NULL|7087500|  2024-12-24|  7730.6|2024|\n",
      "|  4504021|7087520|  2020-02-19|14822.82|2020|\n",
      "|  4504022|7087520|  2020-03-20| 6668.04|2020|\n",
      "|  4504023|7087520|  2020-04-19|  5948.5|2020|\n",
      "|  4504024|7087520|  2020-05-19| 6760.61|2020|\n",
      "|  4504025|7087520|  2020-06-18|12243.51|2020|\n",
      "|  4504026|7087520|  2020-07-18| 3069.33|2020|\n",
      "+---------+-------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_50null = df_50null.withColumn(\"YEAR\", year(col('salary_month')))\n",
    "print(df_50null.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e0b4a-c61d-4781-9d6e-f97ce46080a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
