{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1665b1a4-e352-4c83-8cfc-46972ce53c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|\n",
      "+--------------+-------+------+---------+----------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import col, upper, lower, length, concat_ws, regexp_replace, trim, lpad, rpad, substring\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9c1ae-f639-4d95-a40b-6b18d2490a69",
   "metadata": {},
   "source": [
    "## apply date_format function on string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84095b6b-57ed-4a57-9b58-8f0702d09d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+----+\n",
      "|transaction_id|user_id|amount| location|transaction_time|date|\n",
      "+--------------+-------+------+---------+----------------+----+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|NULL|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|NULL|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|NULL|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|NULL|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|NULL|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|NULL|\n",
      "+--------------+-------+------+---------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, date_format\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df.withColumn(\"date\",date_format(col('transaction_time'), 'MM-yyyy-dd HH:mm')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb5aea-5fae-49a0-9bf8-92e2a45ec22d",
   "metadata": {},
   "source": [
    "## Convert string column to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788a95ab-ce6e-4950-9743-91d6f1ed51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"date\",to_date(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d151fe5-786b-418d-ab1e-f29a7416cf22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_datetime' from 'pyspark.sql.functions' (C:\\Users\\Navya\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_date, to_datetime\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mjoins\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124muser_transactions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,to_date_time(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_time\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMM-dd-yyyy HH:mm\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_datetime' from 'pyspark.sql.functions' (C:\\Users\\Navya\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_datetime\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"date\",to_date_time(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fcbf11b-ca46-4f3c-98fb-93a82ea73942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['curdate', 'current_date', 'date_add', 'date_diff', 'date_format', 'date_from_unix_date', 'date_part', 'date_sub', 'date_trunc', 'dateadd', 'datediff', 'datepart', 'make_date', 'to_date', 'unix_date']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "print([i for i in dir(functions) if 'date' in i.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c237916-33c3-4df5-be6c-94fdee4761ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convert_timezone', 'current_timestamp', 'current_timezone', 'from_unixtime', 'from_utc_timestamp', 'localtimestamp', 'make_timestamp', 'make_timestamp_ltz', 'make_timestamp_ntz', 'timestamp_micros', 'timestamp_millis', 'timestamp_seconds', 'to_timestamp', 'to_timestamp_ltz', 'to_timestamp_ntz', 'to_unix_timestamp', 'to_utc_timestamp', 'try_to_timestamp', 'unix_timestamp', 'window_time']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "print([i for i in dir(functions) if 'time' in i.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff58fe1b-43cb-48e7-b9e5-5cad3e7a89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|               date|\n",
      "+--------------+-------+------+---------+----------------+-------------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03 12:00:00|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03 12:04:00|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03 14:00:00|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03 14:01:00|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03 15:30:00|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03 15:34:00|\n",
      "+--------------+-------+------+---------+----------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"date\",to_timestamp(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73cd7898-0aed-4716-80a9-8d106f28e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------------------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       to_timestamp|   to_date|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03 12:00:00|2025-10-03|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03 12:04:00|2025-10-03|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03 14:00:00|2025-10-03|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03 14:01:00|2025-10-03|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03 15:30:00|2025-10-03|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03 15:34:00|2025-10-03|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"to_timestamp\",to_timestamp(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"to_date\",to_date(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9e1a2e-4254-404f-9227-2655651c02bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       to_timestamp|   to_date|add_2_to_date|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03 12:00:00|2025-10-03|   2025-10-05|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03 12:04:00|2025-10-03|   2025-10-05|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03 14:00:00|2025-10-03|   2025-10-05|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03 14:01:00|2025-10-03|   2025-10-05|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03 15:30:00|2025-10-03|   2025-10-05|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03 15:34:00|2025-10-03|   2025-10-05|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- add_2_to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, date_add\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"to_timestamp\",to_timestamp(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"to_date\",to_date(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_to_date\",date_add(col('to_date'), 2))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36117c50-65cb-4a42-80c2-8d666425afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       to_timestamp|   to_date|add_2_to_date|sub_2_to_date|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03 12:00:00|2025-10-03|   2025-10-05|   2025-10-01|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03 12:04:00|2025-10-03|   2025-10-05|   2025-10-01|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03 14:00:00|2025-10-03|   2025-10-05|   2025-10-01|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03 14:01:00|2025-10-03|   2025-10-05|   2025-10-01|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03 15:30:00|2025-10-03|   2025-10-05|   2025-10-01|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03 15:34:00|2025-10-03|   2025-10-05|   2025-10-01|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- add_2_to_date: date (nullable = true)\n",
      " |-- sub_2_to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, date_add, date_sub\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"to_timestamp\",to_timestamp(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"to_date\",to_date(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_to_date\",date_add(col('to_date'), 2))\n",
    "df = df.withColumn(\"sub_2_to_date\",date_sub(col('to_date'), 2))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67ce4933-2b26-4402-afb5-c90384c91e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+---------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       to_timestamp|   to_date|add_2_to_date|sub_2_to_date|date_diff|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+---------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03 12:00:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03 12:04:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03 14:00:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03 14:01:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03 15:30:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03 15:34:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+---------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- add_2_to_date: date (nullable = true)\n",
      " |-- sub_2_to_date: date (nullable = true)\n",
      " |-- date_diff: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, date_add, date_sub, date_diff\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"to_timestamp\",to_timestamp(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"to_date\",to_date(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_to_date\",date_add(col('to_date'), 2))\n",
    "df = df.withColumn(\"sub_2_to_date\",date_sub(col('to_date'), 2))\n",
    "df = df.withColumn(\"date_diff\", date_diff(\"to_date\", \"add_2_to_date\"))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e2aea08-e3dd-471e-af74-322dc1c29abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+---------+----------+\n",
      "|transaction_id|user_id|amount| location|transaction_time|       to_timestamp|   to_date|add_2_to_date|sub_2_to_date|date_diff|time_stamp|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+---------+----------+\n",
      "|           201|      1|   500|Hyderabad|10-03-2025 12:00|2025-10-03 12:00:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|        -2|\n",
      "|           202|      1|   700|Hyderabad|10-03-2025 12:04|2025-10-03 12:04:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|        -2|\n",
      "|           203|      2|   200|Hyderabad|10-03-2025 14:00|2025-10-03 14:00:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|        -2|\n",
      "|           204|      2|   250|  Chennai|10-03-2025 14:01|2025-10-03 14:01:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|        -2|\n",
      "|           205|      3|  1000| Banglore|10-03-2025 15:30|2025-10-03 15:30:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|        -2|\n",
      "|           206|      3|  1500|Hyderabad|10-03-2025 15:34|2025-10-03 15:34:00|2025-10-03|   2025-10-05|   2025-10-01|       -2|        -2|\n",
      "+--------------+-------+------+---------+----------------+-------------------+----------+-------------+-------------+---------+----------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- transaction_time: string (nullable = true)\n",
      " |-- to_timestamp: timestamp (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      " |-- add_2_to_date: date (nullable = true)\n",
      " |-- sub_2_to_date: date (nullable = true)\n",
      " |-- date_diff: integer (nullable = true)\n",
      " |-- time_stamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, date_add, date_sub, date_diff\n",
    "df = spark.read.csv(\"data\\\\joins\\\\user_transactions.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"to_timestamp\",to_timestamp(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"to_date\",to_date(col('transaction_time'), 'MM-dd-yyyy HH:mm'))\n",
    "df = df.withColumn(\"add_2_to_date\",date_add(col('to_date'), 2))\n",
    "df = df.withColumn(\"sub_2_to_date\",date_sub(col('to_date'), 2))\n",
    "df = df.withColumn(\"date_diff\", date_diff(\"to_date\", \"add_2_to_date\"))\n",
    "df = df.withColumn(\"time_stamp_diff\", date_diff(\"to_date\", \"add_2_to_date\"))\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892faf4-d08c-4a76-ba5e-9a68d7e62fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
